{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c660ba14",
   "metadata": {},
   "source": [
    "# CGCNN Training and Visualization\n",
    "\n",
    "This notebook implements the complete training pipeline for **Crystal Graph Convolutional Neural Networks (CGCNN)**, a deep learning framework for predicting material properties directly from crystal structures.\n",
    "\n",
    "## Reference\n",
    "**Paper**: Xie, T., & Grossman, J. C. (2018). \"Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties.\" *Physical Review Letters*, 120(14), 145301.\n",
    "\n",
    "## Overview\n",
    "\n",
    "CGCNN treats crystal structures as graphs where:\n",
    "- **Nodes**: Represent atoms in the crystal\n",
    "- **Edges**: Represent interactions between neighboring atoms\n",
    "- **Node Features**: Atom properties (element type, electron configuration, etc.)\n",
    "- **Edge Features**: Interatomic distances encoded as Gaussian basis functions\n",
    "\n",
    "The network learns to predict material properties (e.g., formation energy, band gap) through:\n",
    "1. **Graph Construction**: Converting crystal structures to graph representations\n",
    "2. **Message Passing**: Aggregating information from neighboring atoms through multiple convolutional layers\n",
    "3. **Pooling**: Combining atom-level features into crystal-level representation\n",
    "4. **Prediction**: Mapping crystal features to target properties\n",
    "\n",
    "## Workflow\n",
    "1. Load the dataset created in notebook 02\n",
    "2. Define data loading and model architectures\n",
    "3. Train the model with proper tracking and validation\n",
    "4. Evaluate performance on test set\n",
    "5. Visualize training progress and prediction quality\n",
    "6. Save model checkpoints and results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c20a8",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9982741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully\n",
      "PyTorch Version: 2.9.0+cu128\n",
      "CUDA Available: True\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import functools\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Pymatgen\n",
    "from pymatgen.core.structure import Structure\n",
    "from pymatgen.core.periodic_table import Element\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e9067",
   "metadata": {},
   "source": [
    "## Part 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18c42fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  data Directory: ..\\data\n",
      "  Batch Size: 8\n",
      "  Train/Val/Test Split: 0.7/0.15/0.15\n",
      "  Model: 3 conv layers, 64D atom features, 128D hidden\n",
      "  Output Directory: training_results\n"
     ]
    }
   ],
   "source": [
    "# Dataset Configuration\n",
    "DATA_DIR = os.path.join('..', 'data')\n",
    "MAX_NUM_NBR = 12\n",
    "RADIUS = 8.0\n",
    "DMIN = 0\n",
    "DSTEP = 0.2\n",
    "\n",
    "# Data Loading Configuration\n",
    "BATCH_SIZE = 8\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Model Configuration\n",
    "ATOM_FEA_LEN = 64\n",
    "N_CONV = 3\n",
    "H_FEA_LEN = 128\n",
    "N_H = 1\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "WEIGHT_DECAY = 0.0\n",
    "EPOCHS = 5\n",
    "EARLY_STOPPING_PATIENCE = 30\n",
    "\n",
    "# Output Configuration\n",
    "OUTPUT_DIR = 'training_results'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  data Directory: {DATA_DIR}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Train/Val/Test Split: {TRAIN_RATIO}/{VAL_RATIO}/{TEST_RATIO}\")\n",
    "print(f\"  Model: {N_CONV} conv layers, {ATOM_FEA_LEN}D atom features, {H_FEA_LEN}D hidden\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef31264",
   "metadata": {},
   "source": [
    "## Part 3: Data Loading Classes\n",
    "\n",
    "These classes handle crystal structure loading and graph construction.\n",
    "\n",
    "### Theoretical Background\n",
    "\n",
    "#### 1. Gaussian Distance Expansion\n",
    "Interatomic distances are encoded using **Gaussian basis functions** to create continuous, differentiable edge features:\n",
    "\n",
    "$$\n",
    "e_{ij}^k = \\exp\\left(-\\frac{(d_{ij} - \\mu_k)^2}{\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $d_{ij}$: Distance between atoms $i$ and $j$\n",
    "- $\\mu_k$: Center of the $k$-th Gaussian filter\n",
    "- $\\sigma$: Width of the Gaussian (controls smoothness)\n",
    "\n",
    "This expansion converts a single distance value into a vector of features, allowing the network to learn distance-dependent interactions.\n",
    "\n",
    "#### 2. Atom Feature Initialization\n",
    "Each atom is represented by a learnable feature vector based on its element type. These initial features capture:\n",
    "- Element properties (atomic number, electronegativity, etc.)\n",
    "- Electron configuration\n",
    "- Chemical characteristics\n",
    "\n",
    "#### 3. Crystal Graph Dataset\n",
    "The dataset loads crystal structures from CIF (Crystallographic Information File) format and constructs graph representations:\n",
    "- **Input**: CIF files + target properties (e.g., formation energy)\n",
    "- **Output**: Graph tuple (atom_features, edge_features, edge_indices) + target value\n",
    "\n",
    "#### 4. Batching Strategy\n",
    "Since crystals have varying numbers of atoms, we use a custom collating function that:\n",
    "1. Concatenates all atoms from different crystals into a single large graph\n",
    "2. Maintains indices to track which atoms belong to which crystal\n",
    "3. Enables efficient batch processing on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50101ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDistance(object):\n",
    "    \"\"\"\n",
    "    Expands interatomic distances using Gaussian basis functions.\n",
    "    \n",
    "    This class converts scalar distances into vector representations by applying\n",
    "    multiple Gaussian filters centered at different positions along the distance range.\n",
    "    \n",
    "    Mathematical Formulation:\n",
    "        g_k(d) = exp(-(d - μ_k)^2 / σ^2)\n",
    "    \n",
    "    where:\n",
    "        - d: interatomic distance\n",
    "        - μ_k: center of k-th Gaussian filter\n",
    "        - σ: width (standard deviation) of Gaussian\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dmin : float\n",
    "        Minimum distance for Gaussian filters\n",
    "    dmax : float\n",
    "        Maximum distance for Gaussian filters\n",
    "    step : float\n",
    "        Spacing between adjacent Gaussian filter centers\n",
    "    var : float, optional\n",
    "        Variance of Gaussian filters. If None, defaults to step size.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    filter : np.ndarray\n",
    "        Array of Gaussian filter centers, shape (n_filters,)\n",
    "    var : float\n",
    "        Standard deviation of Gaussian filters\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> gdf = GaussianDistance(dmin=0, dmax=8, step=0.2)\n",
    "    >>> distances = np.array([2.5, 3.0, 4.5])  # Shape: (3,)\n",
    "    >>> expanded = gdf.expand(distances)       # Shape: (3, 41)\n",
    "    \"\"\"\n",
    "    def __init__(self, dmin, dmax, step, var=None):\n",
    "        assert dmin < dmax\n",
    "        assert dmax - dmin > step\n",
    "        self.filter = np.arange(dmin, dmax+step, step)  # 1D array of \"center\" points\n",
    "        if var is None:\n",
    "            var = step\n",
    "        self.var = var\n",
    "\n",
    "    def expand(self, distances):\n",
    "        \"\"\"\n",
    "        Apply Gaussian distance filter to distance array.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        distances : np.ndarray\n",
    "            Array of interatomic distances, shape (..., )\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Gaussian-expanded features, shape (..., n_filters)\n",
    "            where n_filters = len(self.filter)\n",
    "        \"\"\"\n",
    "        diff = distances[..., np.newaxis] - self.filter  # Shape (..., n_filters)\n",
    "        squared_diff = diff ** 2\n",
    "        var_squared = self.var ** 2\n",
    "        result = np.exp(-squared_diff / var_squared)\n",
    "        return result\n",
    "\n",
    "class AtomInitializer(object):\n",
    "    \"\"\"\n",
    "    Base class for initializing atom feature vectors.\n",
    "    \n",
    "    This class maintains a mapping from atom types (typically atomic numbers)\n",
    "    to their corresponding feature vectors. Subclasses implement specific\n",
    "    initialization strategies (e.g., from JSON files, random initialization).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    atom_types : set or list\n",
    "        Set of atom types (e.g., atomic numbers) present in the dataset\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    atom_types : set\n",
    "        Set of valid atom types\n",
    "    _embedding : dict\n",
    "        Dictionary mapping atom types to feature vectors\n",
    "    \"\"\"\n",
    "    def __init__(self, atom_types):\n",
    "        self.atom_types = set(atom_types)\n",
    "        self._embedding = {}\n",
    "\n",
    "    def get_atom_fea(self, atom_type):\n",
    "        \"\"\"\n",
    "        Retrieve feature vector for a specific atom type.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        atom_type : int or str\n",
    "            Atom type identifier (e.g., atomic number)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Feature vector for the atom type\n",
    "        \"\"\"\n",
    "        assert atom_type in self.atom_types\n",
    "        return self._embedding[atom_type]\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Load atom embeddings from a state dictionary.\"\"\"\n",
    "        self._embedding = state_dict\n",
    "        self.atom_types = set(self._embedding.keys())\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Return the current embedding dictionary.\"\"\"\n",
    "        return self._embedding\n",
    "\n",
    "class AtomCustomJSONInitializer(AtomInitializer):\n",
    "    \"\"\"\n",
    "    Initialize atom features from a JSON file.\n",
    "    \n",
    "    Loads pre-computed atom embeddings from a JSON file that maps element\n",
    "    symbols or atomic numbers to feature vectors. These embeddings can\n",
    "    encode various atomic properties like electronegativity, ionic radius,\n",
    "    electron affinity, etc.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    elem_embedding_file : str\n",
    "        Path to JSON file containing element embeddings\n",
    "        Format: {element_id: [feature1, feature2, ...], ...}\n",
    "        where element_id can be atomic number (int) or symbol (str)\n",
    "    \n",
    "    Example JSON Structure\n",
    "    ----------------------\n",
    "    {\n",
    "        \"1\": [1.0, 0.5, ...],    # Hydrogen (by atomic number)\n",
    "        \"H\": [1.0, 0.5, ...],    # Hydrogen (by symbol)\n",
    "        \"6\": [2.55, 0.7, ...],   # Carbon\n",
    "        ...\n",
    "    }\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - Automatically converts element symbols to atomic numbers\n",
    "    - Handles both string and integer keys in JSON\n",
    "    - Feature vectors are converted to numpy arrays with dtype=float\n",
    "    \"\"\"\n",
    "    def __init__(self, elem_embedding_file):\n",
    "        with open(elem_embedding_file) as f:\n",
    "            elem_embedding = json.load(f)\n",
    "        \n",
    "        converted_embedding = {}\n",
    "        for key, value in elem_embedding.items():\n",
    "            try:\n",
    "                atomic_num = int(key)\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    atomic_num = Element(key).Z\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not process element '{key}': {e}\")\n",
    "                    continue\n",
    "            converted_embedding[atomic_num] = value\n",
    "        \n",
    "        atom_types = set(converted_embedding.keys())\n",
    "        super(AtomCustomJSONInitializer, self).__init__(atom_types)\n",
    "        \n",
    "        for key, value in converted_embedding.items():\n",
    "            self._embedding[key] = np.array(value, dtype=float)\n",
    "\n",
    "class CIFData(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for crystal structures stored as CIF files.\n",
    "    \n",
    "    This dataset loads crystal structures from CIF files and converts them\n",
    "    into graph representations suitable for CGCNN. Each crystal is represented\n",
    "    as a graph where atoms are nodes and edges connect atoms within a cutoff radius.\n",
    "    \n",
    "    Directory Structure Expected\n",
    "    -----------------------------\n",
    "    data(..)\n",
    "    ├── cgcnn_dataset.csv    # CSV with material_id,cif_file,formula,formation_energy_per_atom,energy_above_hull,band_gap,density,volume,target\n",
    "    ├── atom_embedding.json  # JSON with atom feature embeddings\n",
    "    ├── cif/                 # Folder containing CIF files\n",
    "    │   ├── material1.cif    # CIF file for material 1\n",
    "    │   ├── material2.cif    # CIF file for material 2\n",
    "    │   └── ...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    root_dir : str\n",
    "        Path to directory containing CIF folder having cif id data and metadata\n",
    "    max_num_nbr : int, default=12\n",
    "        Maximum number of neighbors to consider for each atom\n",
    "    radius : float, default=8\n",
    "        Cutoff radius (Angstroms) for neighbor search\n",
    "    dmin : float, default=0\n",
    "        Minimum distance for Gaussian expansion\n",
    "    step : float, default=0.2\n",
    "        Step size for Gaussian filter centers\n",
    "    random_seed : int, default=123\n",
    "        Random seed for shuffling dataset\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    root_dir : str\n",
    "        Root directory path\n",
    "    max_num_nbr : int\n",
    "        Maximum neighbors per atom\n",
    "    radius : float\n",
    "        Neighbor search cutoff radius\n",
    "    ari : AtomCustomJSONInitializer\n",
    "        Atom feature initializer\n",
    "    gdf : GaussianDistance\n",
    "        Gaussian distance expansion object\n",
    "    id_prop_data : list\n",
    "        List of [material_id, target_value] pairs\n",
    "    \n",
    "    Returns (via __getitem__)\n",
    "    --------------------------\n",
    "    tuple : ((atom_fea, nbr_fea, nbr_fea_idx), target, cif_id)\n",
    "        atom_fea : torch.Tensor, shape (n_atoms, atom_fea_len)\n",
    "            Atom feature matrix\n",
    "        nbr_fea : torch.Tensor, shape (n_atoms, max_num_nbr, nbr_fea_len)\n",
    "            Neighbor feature tensor (Gaussian-expanded distances)\n",
    "        nbr_fea_idx : torch.LongTensor, shape (n_atoms, max_num_nbr)\n",
    "            Neighbor indices for each atom\n",
    "        target : torch.Tensor, shape (1,)\n",
    "            Target property value\n",
    "        cif_id : str\n",
    "            Material identifier\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> dataset = CIFData('path/to/data/cif', max_num_nbr=12, radius=8.0)\n",
    "    >>> (atom_fea, nbr_fea, nbr_fea_idx), target, cif_id = dataset[0]\n",
    "    >>> print(f\"Crystal {cif_id} has {atom_fea.shape[0]} atoms\")\n",
    "    >>> print(f\"Target property: {target.item():.4f}\")\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, max_num_nbr=12, radius=8, dmin=0, step=0.2, random_seed=123):\n",
    "        self.root_dir = root_dir\n",
    "        self.max_num_nbr, self.radius = max_num_nbr, radius\n",
    "        \n",
    "        assert os.path.exists(root_dir), f'root_dir does not exist: {root_dir}'\n",
    "\n",
    "        id_prop_file = os.path.join(self.root_dir,'cgcnn_dataset.csv')\n",
    "        assert os.path.exists(id_prop_file), f'cgcnn_dataset.csv not found at {id_prop_file}'\n",
    "\n",
    "        with open(id_prop_file) as f:\n",
    "            reader = csv.reader(f)\n",
    "            # skip header\n",
    "            header = next(reader)\n",
    "            self.id_prop_data = [(row[0], float(row[3])) for row in reader]\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(self.id_prop_data)\n",
    "        \n",
    "        # Load atom embeddings from atom_embedding.json in root_dir\n",
    "        atom_init_file = os.path.join(self.root_dir, 'atom_embedding.json')\n",
    "        assert os.path.exists(atom_init_file), f'atom_embedding.json not found at {atom_init_file}'\n",
    "        \n",
    "        self.ari = AtomCustomJSONInitializer(atom_init_file)\n",
    "        self.gdf = GaussianDistance(dmin=dmin, dmax=self.radius, step=step)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of crystals in the dataset.\"\"\"\n",
    "        return len(self.id_prop_data)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single crystal graph and its target property.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the crystal in the dataset\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tuple : ((atom_fea, nbr_fea, nbr_fea_idx), target, cif_id)\n",
    "            Graph representation and target value\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        - Uses LRU cache to avoid reloading the same crystal multiple times\n",
    "        - Pads neighbor lists to max_num_nbr if insufficient neighbors found\n",
    "        - Applies Gaussian expansion to neighbor distances\n",
    "        \"\"\"\n",
    "        cif_id, target = self.id_prop_data[idx]\n",
    "        # Look for CIF file in cif subdirectory\n",
    "        cif_path = os.path.join(self.root_dir, 'cif', cif_id+'.cif')\n",
    "        crystal = Structure.from_file(cif_path)\n",
    "        \n",
    "        # Get atom features\n",
    "        atom_fea = np.vstack([self.ari.get_atom_fea(crystal[i].specie.number)\n",
    "                              for i in range(len(crystal))])\n",
    "        atom_fea = torch.Tensor(atom_fea)\n",
    "        \n",
    "        # Get neighbors within cutoff radius\n",
    "        all_nbrs = crystal.get_all_neighbors(self.radius, include_index=True)\n",
    "        all_nbrs = [sorted(nbrs, key=lambda x: x[1]) for nbrs in all_nbrs]\n",
    "        \n",
    "        # Build neighbor arrays\n",
    "        nbr_fea_idx, nbr_fea = [], []\n",
    "        for nbr in all_nbrs:\n",
    "            if len(nbr) < self.max_num_nbr:\n",
    "                warnings.warn(f'{cif_id}: not enough neighbors')\n",
    "                nbr_fea_idx.append(list(map(lambda x: x[2], nbr)) + [0] * (self.max_num_nbr - len(nbr)))\n",
    "                nbr_fea.append(list(map(lambda x: x[1], nbr)) + [self.radius + 1.] * (self.max_num_nbr - len(nbr)))\n",
    "            else:\n",
    "                nbr_fea_idx.append(list(map(lambda x: x[2], nbr[:self.max_num_nbr])))\n",
    "                nbr_fea.append(list(map(lambda x: x[1], nbr[:self.max_num_nbr])))\n",
    "        \n",
    "        nbr_fea_idx, nbr_fea = np.array(nbr_fea_idx), np.array(nbr_fea)\n",
    "        nbr_fea = self.gdf.expand(nbr_fea)\n",
    "        \n",
    "        return (torch.Tensor(atom_fea), torch.Tensor(nbr_fea), torch.LongTensor(nbr_fea_idx)), \\\n",
    "               torch.Tensor([float(target)]), cif_id\n",
    "\n",
    "def collate_pool(dataset_list):\n",
    "    \"\"\"\n",
    "    Collate function for batching variable-sized crystal graphs.\n",
    "    \n",
    "    This function merges multiple crystals into a single large graph by:\n",
    "    1. Concatenating all atom features\n",
    "    2. Concatenating all neighbor features\n",
    "    3. Adjusting neighbor indices to account for the combined graph\n",
    "    4. Maintaining mappings from crystals to their constituent atoms\n",
    "    \n",
    "    Theoretical Background\n",
    "    ----------------------\n",
    "    Since crystals have different numbers of atoms, we cannot simply stack them\n",
    "    into a tensor. Instead, we create a \"batch graph\" by concatenating all atoms\n",
    "    and tracking which atoms belong to which crystal. This allows efficient\n",
    "    parallel processing on GPU while handling variable-sized inputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_list : list of tuples\n",
    "        List of dataset items, where each item is:\n",
    "        ((atom_fea, nbr_fea, nbr_fea_idx), target, cif_id)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple : ((batch_atom_fea, batch_nbr_fea, batch_nbr_fea_idx, crystal_atom_idx), \n",
    "             batch_target, batch_cif_ids)\n",
    "        \n",
    "        batch_atom_fea : torch.Tensor, shape (total_atoms, atom_fea_len)\n",
    "            Concatenated atom features from all crystals in batch\n",
    "        \n",
    "        batch_nbr_fea : torch.Tensor, shape (total_atoms, max_num_nbr, nbr_fea_len)\n",
    "            Concatenated neighbor features\n",
    "        \n",
    "        batch_nbr_fea_idx : torch.LongTensor, shape (total_atoms, max_num_nbr)\n",
    "            Concatenated and adjusted neighbor indices\n",
    "        \n",
    "        crystal_atom_idx : list of torch.LongTensor\n",
    "            List where crystal_atom_idx[i] contains indices of atoms\n",
    "            belonging to crystal i in the batch\n",
    "        \n",
    "        batch_target : torch.Tensor, shape (batch_size, 1)\n",
    "            Stacked target values\n",
    "        \n",
    "        batch_cif_ids : list of str\n",
    "            Material identifiers\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    Suppose we have 2 crystals:\n",
    "    - Crystal A: 10 atoms (indices 0-9 in batch)\n",
    "    - Crystal B: 15 atoms (indices 10-24 in batch)\n",
    "    \n",
    "    Then crystal_atom_idx = [\n",
    "        torch.LongTensor([0,1,2,3,4,5,6,7,8,9]),      # Crystal A\n",
    "        torch.LongTensor([10,11,12,13,14,15,...,24])  # Crystal B\n",
    "    ]\n",
    "    \"\"\"\n",
    "    batch_atom_fea, batch_nbr_fea, batch_nbr_fea_idx = [], [], []\n",
    "    crystal_atom_idx, batch_target = [], []\n",
    "    batch_cif_ids = []\n",
    "    base_idx = 0\n",
    "    \n",
    "    for i, ((atom_fea, nbr_fea, nbr_fea_idx), target, cif_id) in enumerate(dataset_list):\n",
    "        n_i = atom_fea.shape[0]  # Number of atoms in this crystal\n",
    "        batch_atom_fea.append(atom_fea)\n",
    "        batch_nbr_fea.append(nbr_fea)\n",
    "        batch_nbr_fea_idx.append(nbr_fea_idx + base_idx)  # Adjust indices\n",
    "        \n",
    "        new_idx = torch.LongTensor(np.arange(n_i) + base_idx)\n",
    "        crystal_atom_idx.append(new_idx)\n",
    "        batch_target.append(target)\n",
    "        batch_cif_ids.append(cif_id)\n",
    "        base_idx += n_i\n",
    "    \n",
    "    return (torch.cat(batch_atom_fea, dim=0),\n",
    "            torch.cat(batch_nbr_fea, dim=0),\n",
    "            torch.cat(batch_nbr_fea_idx, dim=0),\n",
    "            crystal_atom_idx), \\\n",
    "           torch.stack(batch_target, dim=0), \\\n",
    "           batch_cif_ids\n",
    "\n",
    "def get_train_val_test_loader(dataset, collate_fn=default_collate,\n",
    "                              batch_size=64, train_ratio=None,\n",
    "                              val_ratio=0.1, test_ratio=0.1, return_test=False,\n",
    "                              num_workers=0, pin_memory=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Create train, validation, and test data loaders with proper splitting.\n",
    "    \n",
    "    Splits the dataset into training, validation, and test sets, and creates\n",
    "    PyTorch DataLoader objects for each. Uses SubsetRandomSampler to ensure\n",
    "    consistent splits across epochs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : Dataset\n",
    "        PyTorch dataset to split\n",
    "    collate_fn : callable, default=default_collate\n",
    "        Function to merge list of samples into mini-batch\n",
    "        For CGCNN, should be collate_pool\n",
    "    batch_size : int, default=64\n",
    "        Number of crystals per batch\n",
    "    train_ratio : float, optional\n",
    "        Fraction of data for training. If None, computed as 1 - val - test\n",
    "    val_ratio : float, default=0.1\n",
    "        Fraction of data for validation\n",
    "    test_ratio : float, default=0.1\n",
    "        Fraction of data for testing\n",
    "    return_test : bool, default=False\n",
    "        Whether to return test loader\n",
    "    num_workers : int, default=0\n",
    "        Number of subprocesses for data loading (0 = main process only)\n",
    "    pin_memory : bool, default=False\n",
    "        If True, DataLoader will copy tensors into CUDA pinned memory\n",
    "    **kwargs : dict\n",
    "        Additional arguments (train_size, val_size, test_size can override ratios)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (train_loader, val_loader) if return_test=False\n",
    "        (train_loader, val_loader, test_loader) if return_test=True\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - Data is split sequentially (not random), so shuffle your dataset first\n",
    "    - Samplers use indices to select subsets without copying data\n",
    "    - pin_memory=True can speed up CPU->GPU transfer but uses more memory\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> train_loader, val_loader, test_loader = get_train_val_test_loader(\n",
    "    ...     dataset, collate_fn=collate_pool, batch_size=32,\n",
    "    ...     train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, return_test=True\n",
    "    ... )\n",
    "    \"\"\"\n",
    "    total_size = len(dataset)\n",
    "    if train_ratio is None:\n",
    "        train_ratio = 1 - val_ratio - test_ratio\n",
    "    \n",
    "    indices = list(range(total_size))\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    test_size = int(test_ratio * total_size)\n",
    "    valid_size = int(val_ratio * total_size)\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(indices[:train_size])\n",
    "    val_sampler = SubsetRandomSampler(indices[-(valid_size + test_size):-test_size])\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                              num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler,\n",
    "                            num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory)\n",
    "    \n",
    "    if return_test:\n",
    "        test_sampler = SubsetRandomSampler(indices[-test_size:])\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler,\n",
    "                                 num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory)\n",
    "        return train_loader, val_loader, test_loader\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483dad1c",
   "metadata": {},
   "source": [
    "## Part 4: Load Dataset and Create Data Loaders\n",
    "\n",
    "### What Happens Here\n",
    "\n",
    "This section:\n",
    "1. **Loads the crystal dataset** from CIF files created in notebook 02\n",
    "2. **Extracts feature dimensions** needed to initialize the model\n",
    "3. **Creates data loaders** for efficient batch processing during training\n",
    "\n",
    "### Key Information Extracted\n",
    "\n",
    "- **ORIG_ATOM_FEA_LEN**: Dimension of input atom features (depends on atom_init.json)\n",
    "- **NBR_FEA_LEN**: Dimension of edge features (= number of Gaussian filters)\n",
    "  - Computed as: `(radius - dmin) / step + 1`\n",
    "  - Example: `(8.0 - 0) / 0.2 + 1 = 41`\n",
    "\n",
    "### Data Split Strategy\n",
    "\n",
    "```\n",
    "Total Dataset (100%)\n",
    "    ├── Training Set (70%) → Used for weight updates\n",
    "    ├── Validation Set (15%) → Used for hyperparameter tuning\n",
    "    └── Test Set (15%) → Final performance evaluation\n",
    "```\n",
    "\n",
    "**Important**: Test set is never used during training or model selection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbf32365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing GaussianDistance Function\n",
      "================================================================================\n",
      "✅ GaussianDistance initialized:\n",
      "   Filter centers: 41 points\n",
      "   Distance range: 0 to 8.0 Å\n",
      "   Step size: 0.2 Å\n",
      "   Variance: 0.2\n",
      "\n",
      "   Sample distances: [2.5 3.  4.5 5.5]\n",
      "   Expanded shape: (4, 41)\n",
      "   Sample expanded features (first distance):\n",
      "[1.38511937e-68 3.66905962e-58 1.31532589e-48 6.38150345e-40\n",
      " 4.19009319e-32]\n",
      "\n",
      "================================================================================\n",
      "Loading Dataset with CIFData Class\n",
      "================================================================================\n",
      "✅ Dataset loaded successfully:\n",
      "   Total crystals: 103644\n",
      "   First crystal: mp-1229074\n",
      "   Number of atoms: 60\n",
      "   Atom feature dimension: 186\n",
      "   Max neighbors: 12\n",
      "   Neighbor feature dimension: 41\n",
      "   Target property value: -1.964300\n",
      "\n",
      "================================================================================\n",
      "Creating Data Loaders with get_train_val_test_loader\n",
      "================================================================================\n",
      "✅ Data loaders created successfully:\n",
      "   Training batches: 9069\n",
      "   Training samples: 103644\n",
      "   Validation batches: 1944\n",
      "   Validation samples: 103644\n",
      "   Test batches: 1944\n",
      "   Test samples: 103644\n",
      "\n",
      "================================================================================\n",
      "Testing collate_pool Function\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 36 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 12 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 16 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch collated successfully:\n",
      "   Batch atom features shape: torch.Size([293, 186])\n",
      "   Batch neighbor features shape: torch.Size([293, 12, 41])\n",
      "   Batch neighbor indices shape: torch.Size([293, 12])\n",
      "   Number of crystals in batch: 8\n",
      "   Crystal atom indices (first crystal): tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n",
      "   Batch target shape: torch.Size([8, 1])\n",
      "   Batch material IDs: ['mp-1208706', 'mp-721017', 'mp-5454']...\n",
      "\n",
      "================================================================================\n",
      "Saving Data Loading Summary\n",
      "================================================================================\n",
      "✅ Saved to:\n",
      "   training_results\\data_loading_summary.json\n",
      "   training_results\\data_loading_summary.csv\n",
      "\n",
      "================================================================================\n",
      "Data Loading Pipeline Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test GaussianDistance class\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing GaussianDistance Function\")\n",
    "print(\"=\"*80)\n",
    "gdf_test = GaussianDistance(dmin=DMIN, dmax=RADIUS, step=DSTEP)\n",
    "print(f\"✅ GaussianDistance initialized:\")\n",
    "print(f\"   Filter centers: {len(gdf_test.filter)} points\")\n",
    "print(f\"   Distance range: {DMIN} to {RADIUS} Å\")\n",
    "print(f\"   Step size: {DSTEP} Å\")\n",
    "print(f\"   Variance: {gdf_test.var}\")\n",
    "\n",
    "# Test with sample distances\n",
    "sample_distances = np.array([2.5, 3.0, 4.5, 5.5])\n",
    "expanded_features = gdf_test.expand(sample_distances)\n",
    "print(f\"\\n   Sample distances: {sample_distances}\")\n",
    "print(f\"   Expanded shape: {expanded_features.shape}\")\n",
    "print(f\"   Sample expanded features (first distance):\\n{expanded_features[0, :5]}\")\n",
    "\n",
    "# Load dataset using CIFData class\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading Dataset with CIFData Class\")\n",
    "print(\"=\"*80)\n",
    "dataset = CIFData(\n",
    "    root_dir=DATA_DIR,\n",
    "    max_num_nbr=MAX_NUM_NBR,\n",
    "    radius=RADIUS,\n",
    "    dmin=DMIN,\n",
    "    step=DSTEP,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"✅ Dataset loaded successfully:\")\n",
    "print(f\"   Total crystals: {len(dataset)}\")\n",
    "\n",
    "# Get first sample to verify structure\n",
    "(atom_fea, nbr_fea, nbr_fea_idx), target, cif_id = dataset[0]\n",
    "ORIG_ATOM_FEA_LEN = atom_fea.shape[1]\n",
    "NBR_FEA_LEN = nbr_fea.shape[2]\n",
    "\n",
    "print(f\"   First crystal: {cif_id}\")\n",
    "print(f\"   Number of atoms: {atom_fea.shape[0]}\")\n",
    "print(f\"   Atom feature dimension: {ORIG_ATOM_FEA_LEN}\")\n",
    "print(f\"   Max neighbors: {nbr_fea.shape[1]}\")\n",
    "print(f\"   Neighbor feature dimension: {NBR_FEA_LEN}\")\n",
    "print(f\"   Target property value: {target.item():.6f}\")\n",
    "\n",
    "# Create data loaders using get_train_val_test_loader\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Data Loaders with get_train_val_test_loader\")\n",
    "print(\"=\"*80)\n",
    "train_loader, val_loader, test_loader = get_train_val_test_loader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=collate_pool,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    test_ratio=TEST_RATIO,\n",
    "    return_test=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\"✅ Data loaders created successfully:\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Validation samples: {len(val_loader.dataset)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")\n",
    "print(f\"   Test samples: {len(test_loader.dataset)}\")\n",
    "\n",
    "# Test collate_pool function with first batch\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing collate_pool Function\")\n",
    "print(\"=\"*80)\n",
    "first_batch = next(iter(train_loader))\n",
    "(batch_atom_fea, batch_nbr_fea, batch_nbr_fea_idx, crystal_atom_idx), batch_target, batch_cif_ids = first_batch\n",
    "\n",
    "print(f\"✅ Batch collated successfully:\")\n",
    "print(f\"   Batch atom features shape: {batch_atom_fea.shape}\")\n",
    "print(f\"   Batch neighbor features shape: {batch_nbr_fea.shape}\")\n",
    "print(f\"   Batch neighbor indices shape: {batch_nbr_fea_idx.shape}\")\n",
    "print(f\"   Number of crystals in batch: {len(crystal_atom_idx)}\")\n",
    "print(f\"   Crystal atom indices (first crystal): {crystal_atom_idx[0]}\")\n",
    "print(f\"   Batch target shape: {batch_target.shape}\")\n",
    "print(f\"   Batch material IDs: {batch_cif_ids[:3]}...\")  # Show first 3\n",
    "\n",
    "# Save data loading summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Saving Data Loading Summary\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "data_summary = {\n",
    "    'GaussianDistance': {\n",
    "        'filter_centers': len(gdf_test.filter),\n",
    "        'distance_range': f'{DMIN} to {RADIUS} Å',\n",
    "        'step_size': DSTEP,\n",
    "        'variance': gdf_test.var\n",
    "    },\n",
    "    'Dataset': {\n",
    "        'total_samples': len(dataset),\n",
    "        'atom_feature_dimension': ORIG_ATOM_FEA_LEN,\n",
    "        'neighbor_feature_dimension': NBR_FEA_LEN,\n",
    "        'max_neighbors': MAX_NUM_NBR,\n",
    "        'radius_cutoff': RADIUS\n",
    "    },\n",
    "    'DataLoaders': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'train_ratio': TRAIN_RATIO,\n",
    "        'val_ratio': VAL_RATIO,\n",
    "        'test_ratio': TEST_RATIO,\n",
    "        'train_batches': len(train_loader),\n",
    "        'val_batches': len(val_loader),\n",
    "        'test_batches': len(test_loader),\n",
    "        'train_samples': len(train_loader.dataset),\n",
    "        'val_samples': len(val_loader.dataset),\n",
    "        'test_samples': len(test_loader.dataset)\n",
    "    },\n",
    "    'FirstSample': {\n",
    "        'material_id': cif_id,\n",
    "        'num_atoms': int(atom_fea.shape[0]),\n",
    "        'target_value': float(target.item()),\n",
    "        'atom_fea_shape': list(atom_fea.shape),\n",
    "        'nbr_fea_shape': list(nbr_fea.shape),\n",
    "        'nbr_fea_idx_shape': list(nbr_fea_idx.shape)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open(os.path.join(OUTPUT_DIR, 'data_loading_summary.json'), 'w') as f:\n",
    "    json.dump(data_summary, f, indent=2)\n",
    "\n",
    "# Save to CSV\n",
    "summary_df = pd.DataFrame({\n",
    "    'Component': ['GaussianDistance', 'Dataset', 'DataLoaders', 'FirstSample'],\n",
    "    'Key_Information': [\n",
    "        f\"Filters: {len(gdf_test.filter)}, Range: {DMIN}-{RADIUS}Å, Variance: {gdf_test.var}\",\n",
    "        f\"Samples: {len(dataset)}, Atom Features: {ORIG_ATOM_FEA_LEN}, Neighbor Features: {NBR_FEA_LEN}\",\n",
    "        f\"Train: {len(train_loader)} batches ({len(train_loader.dataset)} samples), Val: {len(val_loader)} batches ({len(val_loader.dataset)} samples), Test: {len(test_loader)} batches ({len(test_loader.dataset)} samples)\",\n",
    "        f\"Crystal: {cif_id}, Atoms: {atom_fea.shape[0]}, Target: {target.item():.6f}\"\n",
    "    ]\n",
    "})\n",
    "summary_df.to_csv(os.path.join(OUTPUT_DIR, 'data_loading_summary.csv'), index=False)\n",
    "\n",
    "print(f\"✅ Saved to:\")\n",
    "print(f\"   {os.path.join(OUTPUT_DIR, 'data_loading_summary.json')}\")\n",
    "print(f\"   {os.path.join(OUTPUT_DIR, 'data_loading_summary.csv')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Data Loading Pipeline Complete!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a496592",
   "metadata": {},
   "source": [
    "## Part 5: CGCNN Model Definition\n",
    "\n",
    "### Theoretical Background\n",
    "\n",
    "#### Graph Convolution on Crystals\n",
    "\n",
    "The CGCNN architecture implements a **message passing neural network** that operates on crystal graphs. The key innovation is adapting convolutional operations to work on irregular graph structures.\n",
    "\n",
    "#### Convolution Layer Mathematics\n",
    "\n",
    "For each atom $i$, the convolution operation aggregates information from its neighbors $\\mathcal{N}(i)$:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_i^{(t+1)} = \\mathbf{v}_i^{(t)} + \\sum_{j \\in \\mathcal{N}(i)} \\sigma\\left(\\mathbf{z}_{ij}^{(t)}\\right) \\odot \\mathbf{g}\\left(\\mathbf{z}_{ij}^{(t)}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{v}_i^{(t)}$ : Feature vector of atom $i$ at layer $t$\n",
    "- $\\mathbf{z}_{ij}^{(t)} = [\\mathbf{v}_i^{(t)} \\oplus \\mathbf{v}_j^{(t)} \\oplus \\mathbf{u}_{ij}]$ : Concatenation of center atom, neighbor, and edge features\n",
    "- $\\sigma$ : Sigmoid gate function (controls information flow)\n",
    "- $\\mathbf{g}$ : Core message function (softplus activation)\n",
    "- $\\odot$ : Element-wise multiplication\n",
    "\n",
    "#### Network Architecture\n",
    "\n",
    "```\n",
    "Crystal Structure (CIF)\n",
    "    ↓\n",
    "[Atom Features + Edge Features]\n",
    "    ↓\n",
    "Embedding Layer → Hidden dimension\n",
    "    ↓\n",
    "Conv Layer 1 → Message passing iteration 1\n",
    "    ↓\n",
    "Conv Layer 2 → Message passing iteration 2\n",
    "    ↓\n",
    "    ...\n",
    "    ↓\n",
    "Conv Layer n → Message passing iteration n\n",
    "    ↓\n",
    "Pooling Layer → Mean over all atoms per crystal\n",
    "    ↓\n",
    "Fully Connected Layers → Property-specific features\n",
    "    ↓\n",
    "Output Layer → Predicted property value\n",
    "```\n",
    "\n",
    "#### Key Components\n",
    "\n",
    "1. **ConvLayer**: Graph convolution with gating mechanism\n",
    "   - Input: Atom features, neighbor features, neighbor indices\n",
    "   - Output: Updated atom features\n",
    "   \n",
    "2. **CrystalGraphConvNet**: Complete CGCNN model\n",
    "   - Embedding: Projects initial atom features to hidden space\n",
    "   - Multiple ConvLayers: Stack of graph convolutions\n",
    "   - Pooling: Aggregates atom-level → crystal-level features\n",
    "   - FC layers: Maps to target property\n",
    "\n",
    "#### Gating Mechanism\n",
    "\n",
    "The sigmoid gate $\\sigma$ acts as an attention mechanism, learning which neighbor information is most relevant:\n",
    "- Gate value ≈ 0: Neighbor has little influence\n",
    "- Gate value ≈ 1: Neighbor has strong influence\n",
    "\n",
    "This allows the network to focus on chemically important interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83db7f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CGCNN model defined\n"
     ]
    }
   ],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph convolutional layer for crystal structures.\n",
    "    \n",
    "    Implements the message passing convolution operation that aggregates\n",
    "    information from neighboring atoms using a gating mechanism. This layer\n",
    "    is the core building block of CGCNN.\n",
    "    \n",
    "    Mathematical Formulation\n",
    "    ------------------------\n",
    "    For each atom i with neighbors j ∈ N(i):\n",
    "    \n",
    "    1. Concatenate features: z_ij = [v_i || v_j || u_ij]\n",
    "    2. Linear transformation: h_ij = W * z_ij + b\n",
    "    3. Split into gate and core: f_ij, c_ij = split(BN(h_ij))\n",
    "    4. Apply activations: f_ij = sigmoid(f_ij), c_ij = softplus(c_ij)\n",
    "    5. Gated aggregation: m_i = Σ_j (f_ij ⊙ c_ij)\n",
    "    6. Residual update: v_i' = softplus(v_i + BN(m_i))\n",
    "    \n",
    "    where:\n",
    "        v_i: central atom features\n",
    "        v_j: neighbor atom features\n",
    "        u_ij: edge features (bond information)\n",
    "        ||: concatenation\n",
    "        ⊙: element-wise multiplication\n",
    "        BN: batch normalization\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    atom_fea_len : int\n",
    "        Dimension of atom feature vectors\n",
    "    nbr_fea_len : int\n",
    "        Dimension of neighbor (edge) feature vectors\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    fc_full : nn.Linear\n",
    "        Fully connected layer mapping concatenated features to gated features\n",
    "        Input: (2*atom_fea_len + nbr_fea_len)\n",
    "        Output: 2*atom_fea_len (split into filter and core)\n",
    "    sigmoid : nn.Sigmoid\n",
    "        Gate activation function (range: 0 to 1)\n",
    "    softplus1, softplus2 : nn.Softplus\n",
    "        Smooth approximation of ReLU for non-linear activations\n",
    "    bn1, bn2 : nn.BatchNorm1d\n",
    "        Batch normalization layers for training stability\n",
    "    \n",
    "    Input Shape\n",
    "    -----------\n",
    "    atom_in_fea : torch.Tensor, shape (N, atom_fea_len)\n",
    "        Atom features before convolution\n",
    "    nbr_fea : torch.Tensor, shape (N, M, nbr_fea_len)\n",
    "        Neighbor features (edge features)\n",
    "    nbr_fea_idx : torch.LongTensor, shape (N, M)\n",
    "        Indices of neighbors for each atom\n",
    "        \n",
    "    where:\n",
    "        N = total number of atoms in batch\n",
    "        M = maximum number of neighbors per atom\n",
    "    \n",
    "    Output Shape\n",
    "    ------------\n",
    "    atom_out_fea : torch.Tensor, shape (N, atom_fea_len)\n",
    "        Updated atom features after convolution\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> conv = ConvLayer(atom_fea_len=64, nbr_fea_len=41)\n",
    "    >>> atom_fea = torch.randn(100, 64)  # 100 atoms\n",
    "    >>> nbr_fea = torch.randn(100, 12, 41)  # each atom has 12 neighbors\n",
    "    >>> nbr_idx = torch.randint(0, 100, (100, 12))\n",
    "    >>> atom_fea_updated = conv(atom_fea, nbr_fea, nbr_idx)\n",
    "    >>> print(atom_fea_updated.shape)  # torch.Size([100, 64])\n",
    "    \"\"\"\n",
    "    def __init__(self, atom_fea_len, nbr_fea_len):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.atom_fea_len = atom_fea_len\n",
    "        self.nbr_fea_len = nbr_fea_len\n",
    "        \n",
    "        self.fc_full = nn.Linear(2*self.atom_fea_len+self.nbr_fea_len, 2*self.atom_fea_len)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus1 = nn.Softplus()\n",
    "        self.softplus2 = nn.Softplus()\n",
    "        self.bn1 = nn.BatchNorm1d(2*self.atom_fea_len)\n",
    "        self.bn2 = nn.BatchNorm1d(self.atom_fea_len)\n",
    "\n",
    "    def forward(self, atom_in_fea, nbr_fea, nbr_fea_idx):\n",
    "        \"\"\"\n",
    "        Forward pass through the graph convolutional layer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        atom_in_fea : torch.Tensor, shape (N, atom_fea_len)\n",
    "            Input atom features\n",
    "        nbr_fea : torch.Tensor, shape (N, M, nbr_fea_len)\n",
    "            Neighbor edge features\n",
    "        nbr_fea_idx : torch.LongTensor, shape (N, M)\n",
    "            Neighbor indices\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor, shape (N, atom_fea_len)\n",
    "            Updated atom features after message passing\n",
    "        \"\"\"\n",
    "        N, M = nbr_fea_idx.shape\n",
    "        \n",
    "        # Gather neighbor atom features\n",
    "        atom_nbr_fea = atom_in_fea[nbr_fea_idx, :]\n",
    "        \n",
    "        # Concatenate: [central_atom || neighbor_atom || edge]\n",
    "        total_nbr_fea = torch.cat(\n",
    "            [atom_in_fea.unsqueeze(1).expand(N, M, self.atom_fea_len),\n",
    "             atom_nbr_fea, nbr_fea], dim=2)\n",
    "        \n",
    "        # Linear transformation + batch norm\n",
    "        total_gated_fea = self.fc_full(total_nbr_fea)\n",
    "        total_gated_fea = self.bn1(total_gated_fea.view(-1, self.atom_fea_len*2)).view(N, M, self.atom_fea_len*2)\n",
    "        \n",
    "        # Split into filter (gate) and core (message)\n",
    "        nbr_filter, nbr_core = total_gated_fea.chunk(2, dim=2)\n",
    "        \n",
    "        # Apply activations\n",
    "        nbr_filter = self.sigmoid(nbr_filter)    # Gate: 0 to 1\n",
    "        nbr_core = self.softplus1(nbr_core)      # Message: >= 0\n",
    "        \n",
    "        # Gated aggregation: sum over neighbors\n",
    "        nbr_sumed = torch.sum(nbr_filter * nbr_core, dim=1)\n",
    "        nbr_sumed = self.bn2(nbr_sumed)\n",
    "        \n",
    "        # Residual connection + activation\n",
    "        out = self.softplus2(atom_in_fea + nbr_sumed)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class CrystalGraphConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Crystal Graph Convolutional Neural Network for material property prediction.\n",
    "    \n",
    "    This model implements the CGCNN architecture described in:\n",
    "    Xie & Grossman (2018), Physical Review Letters, 120(14), 145301.\n",
    "    \n",
    "    The network processes crystal structures as graphs and predicts material\n",
    "    properties through a series of graph convolutions followed by pooling and\n",
    "    fully connected layers.\n",
    "    \n",
    "    Architecture Flow\n",
    "    -----------------\n",
    "    Input: Crystal graph (atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "        ↓\n",
    "    Embedding Layer: orig_atom_fea_len → atom_fea_len\n",
    "        ↓\n",
    "    Conv Layer 1: Graph convolution with message passing\n",
    "        ↓\n",
    "    Conv Layer 2: Graph convolution with message passing\n",
    "        ↓\n",
    "    ... (n_conv layers total)\n",
    "        ↓\n",
    "    Pooling: Atom-level → Crystal-level (mean pooling)\n",
    "        ↓\n",
    "    FC Layer: atom_fea_len → h_fea_len\n",
    "        ↓\n",
    "    [Optional: Additional FC layers]\n",
    "        ↓\n",
    "    Output Layer: h_fea_len → 1 (regression) or 2 (classification)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    orig_atom_fea_len : int\n",
    "        Dimension of input atom features (e.g., 92 for one-hot encoding)\n",
    "    nbr_fea_len : int\n",
    "        Dimension of edge features (e.g., 41 for Gaussian expansion)\n",
    "    atom_fea_len : int, default=64\n",
    "        Hidden dimension for atom features in conv layers\n",
    "    n_conv : int, default=3\n",
    "        Number of graph convolutional layers\n",
    "    h_fea_len : int, default=128\n",
    "        Dimension of hidden features after pooling\n",
    "    n_h : int, default=1\n",
    "        Number of hidden fully connected layers after pooling\n",
    "    classification : bool, default=False\n",
    "        If True, performs classification; if False, performs regression\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    embedding : nn.Linear\n",
    "        Projects input atom features to hidden dimension\n",
    "    convs : nn.ModuleList\n",
    "        List of graph convolutional layers\n",
    "    conv_to_fc : nn.Linear\n",
    "        Transition from convolutional to fully connected layers\n",
    "    fcs : nn.ModuleList (optional)\n",
    "        Additional fully connected hidden layers\n",
    "    fc_out : nn.Linear\n",
    "        Output layer (1 neuron for regression, 2 for binary classification)\n",
    "    \n",
    "    Input Format\n",
    "    ------------\n",
    "    atom_fea : torch.Tensor, shape (N, orig_atom_fea_len)\n",
    "        Atom features from atom types\n",
    "    nbr_fea : torch.Tensor, shape (N, M, nbr_fea_len)\n",
    "        Edge features (Gaussian-expanded distances)\n",
    "    nbr_fea_idx : torch.LongTensor, shape (N, M)\n",
    "        Neighbor indices for each atom\n",
    "    crystal_atom_idx : list of torch.LongTensor\n",
    "        Mapping from crystal index to atom indices\n",
    "        crystal_atom_idx[i] = tensor of atom indices for crystal i\n",
    "    \n",
    "    where:\n",
    "        N = total atoms in batch\n",
    "        M = max neighbors per atom\n",
    "        N0 = number of crystals in batch\n",
    "    \n",
    "    Output Format\n",
    "    -------------\n",
    "    Regression: torch.Tensor, shape (N0, 1)\n",
    "        Predicted property values\n",
    "    Classification: torch.Tensor, shape (N0, 2)\n",
    "        Log probabilities for each class\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> model = CrystalGraphConvNet(\n",
    "    ...     orig_atom_fea_len=92, nbr_fea_len=41,\n",
    "    ...     atom_fea_len=64, n_conv=3, h_fea_len=128\n",
    "    ... )\n",
    "    >>> # Dummy batch with 2 crystals (10 and 15 atoms)\n",
    "    >>> atom_fea = torch.randn(25, 92)\n",
    "    >>> nbr_fea = torch.randn(25, 12, 41)\n",
    "    >>> nbr_fea_idx = torch.randint(0, 25, (25, 12))\n",
    "    >>> crystal_atom_idx = [torch.arange(10), torch.arange(10, 25)]\n",
    "    >>> output = model(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "    >>> print(output.shape)  # torch.Size([2, 1])\n",
    "    \"\"\"\n",
    "    def __init__(self, orig_atom_fea_len, nbr_fea_len,\n",
    "                 atom_fea_len=64, n_conv=3, h_fea_len=128, n_h=1,\n",
    "                 classification=False):\n",
    "        super(CrystalGraphConvNet, self).__init__()\n",
    "        self.classification = classification\n",
    "        \n",
    "        self.embedding = nn.Linear(orig_atom_fea_len, atom_fea_len)\n",
    "        self.convs = nn.ModuleList([ConvLayer(atom_fea_len=atom_fea_len,\n",
    "                                    nbr_fea_len=nbr_fea_len)\n",
    "                                    for _ in range(n_conv)])\n",
    "        self.conv_to_fc = nn.Linear(atom_fea_len, h_fea_len)\n",
    "        self.conv_to_fc_softplus = nn.Softplus()\n",
    "        \n",
    "        if n_h > 1:\n",
    "            self.fcs = nn.ModuleList([nn.Linear(h_fea_len, h_fea_len) for _ in range(n_h-1)])\n",
    "            self.softpluses = nn.ModuleList([nn.Softplus() for _ in range(n_h-1)])\n",
    "        \n",
    "        if self.classification:\n",
    "            self.fc_out = nn.Linear(h_fea_len, 2)\n",
    "            self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "            self.dropout = nn.Dropout()\n",
    "        else:\n",
    "            self.fc_out = nn.Linear(h_fea_len, 1)\n",
    "\n",
    "    def forward(self, atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx):\n",
    "        \"\"\"\n",
    "        Forward pass through the CGCNN model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        atom_fea : torch.Tensor, shape (N, orig_atom_fea_len)\n",
    "            Input atom features\n",
    "        nbr_fea : torch.Tensor, shape (N, M, nbr_fea_len)\n",
    "            Edge features\n",
    "        nbr_fea_idx : torch.LongTensor, shape (N, M)\n",
    "            Neighbor indices\n",
    "        crystal_atom_idx : list of torch.LongTensor\n",
    "            Crystal-to-atom mapping\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            shape (N0, 1) for regression\n",
    "            shape (N0, 2) for classification (log probabilities)\n",
    "        \"\"\"\n",
    "        # Embed atom features\n",
    "        atom_fea = self.embedding(atom_fea)\n",
    "        \n",
    "        # Apply graph convolutions\n",
    "        for conv_func in self.convs:\n",
    "            atom_fea = conv_func(atom_fea, nbr_fea, nbr_fea_idx)\n",
    "        \n",
    "        # Pool to crystal-level features\n",
    "        crys_fea = self.pooling(atom_fea, crystal_atom_idx)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        crys_fea = self.conv_to_fc(self.conv_to_fc_softplus(crys_fea))\n",
    "        crys_fea = self.conv_to_fc_softplus(crys_fea)\n",
    "        \n",
    "        if self.classification:\n",
    "            crys_fea = self.dropout(crys_fea)\n",
    "        \n",
    "        if hasattr(self, 'fcs') and hasattr(self, 'softpluses'):\n",
    "            for fc, softplus in zip(self.fcs, self.softpluses):\n",
    "                crys_fea = softplus(fc(crys_fea))\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc_out(crys_fea)\n",
    "        \n",
    "        if self.classification:\n",
    "            out = self.logsoftmax(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def pooling(self, atom_fea, crystal_atom_idx):\n",
    "        \"\"\"\n",
    "        Pool atom-level features to crystal-level features.\n",
    "        \n",
    "        Uses mean pooling: averages all atom features belonging to each crystal.\n",
    "        This operation is permutation-invariant (order of atoms doesn't matter).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        atom_fea : torch.Tensor, shape (N, atom_fea_len)\n",
    "            Atom feature vectors\n",
    "        crystal_atom_idx : list of torch.LongTensor\n",
    "            Indices mapping crystals to their atoms\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor, shape (N0, atom_fea_len)\n",
    "            Crystal-level feature vectors\n",
    "        \"\"\"\n",
    "        summed_fea = [torch.mean(atom_fea[idx_map], dim=0, keepdim=True)\n",
    "                      for idx_map in crystal_atom_idx]\n",
    "        return torch.cat(summed_fea, dim=0)\n",
    "\n",
    "\n",
    "print(\"✅ CGCNN model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "289dc437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data loaders...\n",
      "✅ Data loaders created:\n",
      "   Training batches: 9069\n",
      "   Validation batches: 1944\n",
      "   Test batches: 1944\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "print(\"Creating data loaders...\")\n",
    "train_loader, val_loader, test_loader = get_train_val_test_loader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=collate_pool,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    test_ratio=TEST_RATIO,\n",
    "    return_test=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\"✅ Data loaders created:\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9332a0c",
   "metadata": {},
   "source": [
    "## Part 6: Initialize Model and Training Components\n",
    "\n",
    "### Model Initialization\n",
    "\n",
    "The model architecture is determined by hyperparameters:\n",
    "- **atom_fea_len**: Controls model capacity (higher = more parameters)\n",
    "- **n_conv**: Number of message passing iterations (deeper = more expressiveness)\n",
    "- **h_fea_len**: Hidden dimension after pooling\n",
    "\n",
    "### Training Components\n",
    "\n",
    "1. **Loss Function (MSE)**\n",
    "   - Appropriate for continuous property prediction\n",
    "   - Penalizes large errors more than MAE\n",
    "   \n",
    "2. **Optimizer (Adam)**\n",
    "   - Adaptive learning rates for each parameter\n",
    "   - Combines benefits of RMSprop and momentum\n",
    "   - Generally converges faster than SGD\n",
    "   \n",
    "3. **Learning Rate Scheduler**\n",
    "   - **ReduceLROnPlateau**: Monitors validation loss\n",
    "   - Reduces LR by factor of 0.5 if no improvement for 10 epochs\n",
    "   - Helps escape local minima and fine-tune convergence\n",
    "\n",
    "### Model Complexity\n",
    "\n",
    "Total parameters ≈ embedding + convolutions + FC layers:\n",
    "- More parameters → More capacity but higher overfitting risk\n",
    "- Typical CGCNN: 50k - 200k parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a4906d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "✅ Model initialized:\n",
      "   Total parameters: 86,849\n",
      "   Loss: MSE\n",
      "   Optimizer: Adam (LR=0.1)\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = CrystalGraphConvNet(\n",
    "    orig_atom_fea_len=ORIG_ATOM_FEA_LEN,\n",
    "    nbr_fea_len=NBR_FEA_LEN,\n",
    "    atom_fea_len=ATOM_FEA_LEN,\n",
    "    n_conv=N_CONV,\n",
    "    h_fea_len=H_FEA_LEN,\n",
    "    n_h=N_H,\n",
    "    classification=False\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "print(f\"\\n✅ Model initialized:\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Loss: MSE\")\n",
    "print(f\"   Optimizer: Adam (LR={LEARNING_RATE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24b04ea",
   "metadata": {},
   "source": [
    "## Part 7: Training Functions\n",
    "\n",
    "### Theoretical Background\n",
    "\n",
    "#### Training Process\n",
    "\n",
    "The training loop implements **supervised learning** where the model learns to minimize the difference between predicted and actual material properties.\n",
    "\n",
    "#### Loss Function (for Regression)\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "$$\n",
    "\\mathcal{L}_{\\text{MSE}} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y_i$ : True property value\n",
    "- $\\hat{y}_i$ : Predicted property value\n",
    "- $N$ : Number of samples in batch\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "The model parameters $\\theta$ are updated using the **Adam optimizer**:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ : Learning rate\n",
    "- $m_t$ : First moment estimate (moving average of gradients)\n",
    "- $v_t$ : Second moment estimate (moving average of squared gradients)\n",
    "- $\\epsilon$ : Small constant for numerical stability\n",
    "\n",
    "#### Evaluation Metrics\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**:\n",
    "   $$\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|$$\n",
    "\n",
    "2. **Root Mean Squared Error (RMSE)**:\n",
    "   $$\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "3. **R² Score (Coefficient of Determination)**:\n",
    "   $$R^2 = 1 - \\frac{\\sum_{i}(y_i - \\hat{y}_i)^2}{\\sum_{i}(y_i - \\bar{y})^2}$$\n",
    "   \n",
    "   where $\\bar{y}$ is the mean of true values. R² = 1 indicates perfect prediction.\n",
    "\n",
    "#### Training vs Validation\n",
    "\n",
    "- **Training Set**: Used to update model weights via backpropagation\n",
    "- **Validation Set**: Used to monitor generalization and tune hyperparameters\n",
    "- **Test Set**: Final evaluation of model performance (never used during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b4ce28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one complete epoch.\n",
    "    \n",
    "    Performs a full pass through the training dataset, computing gradients\n",
    "    and updating model weights via backpropagation. Tracks loss and metrics\n",
    "    throughout the epoch.\n",
    "    \n",
    "    Training Process\n",
    "    ----------------\n",
    "    For each batch:\n",
    "    1. Forward pass: compute predictions\n",
    "    2. Compute loss: compare predictions to targets\n",
    "    3. Backward pass: compute gradients via backpropagation\n",
    "    4. Optimizer step: update model weights\n",
    "    5. Track metrics: accumulate loss, predictions, targets\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        CGCNN model to train\n",
    "    train_loader : DataLoader\n",
    "        DataLoader providing training batches\n",
    "    criterion : nn.Module\n",
    "        Loss function (e.g., nn.MSELoss())\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer for updating weights (e.g., Adam)\n",
    "    device : torch.device\n",
    "        Device to run computations on (CPU or CUDA)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple : (avg_loss, mae, rmse, predictions, targets)\n",
    "        avg_loss : float\n",
    "            Average loss over all training samples\n",
    "        mae : float\n",
    "            Mean Absolute Error on training set\n",
    "        rmse : float\n",
    "            Root Mean Squared Error on training set\n",
    "        predictions : list of float\n",
    "            All predicted values (flattened)\n",
    "        targets : list of float\n",
    "            All true values (flattened)\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - Model is set to training mode (enables dropout, batch norm updates)\n",
    "    - Gradients are computed and weights updated for each batch\n",
    "    - Progress bar shows current batch loss\n",
    "    - All data moved to specified device (GPU if available)\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> train_loss, train_mae, train_rmse, preds, targs = train_epoch(\n",
    "    ...     model, train_loader, criterion, optimizer, device\n",
    "    ... )\n",
    "    >>> print(f\"Training Loss: {train_loss:.4f}, MAE: {train_mae:.4f}\")\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    predictions, targets = [], []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training', leave=False)\n",
    "    for batch_idx, (input_data, target, _) in enumerate(pbar):\n",
    "        atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx = input_data\n",
    "        atom_fea = atom_fea.to(device)\n",
    "        nbr_fea = nbr_fea.to(device)\n",
    "        nbr_fea_idx = nbr_fea_idx.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * target.size(0)\n",
    "        predictions.extend(output.detach().cpu().numpy().flatten())\n",
    "        targets.extend(target.detach().cpu().numpy().flatten())\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(targets, predictions))\n",
    "    return avg_loss, mae, rmse, predictions, targets\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the model for one complete epoch.\n",
    "    \n",
    "    Performs a full pass through the validation/test dataset without updating\n",
    "    weights. Used to monitor model performance and detect overfitting.\n",
    "    \n",
    "    Validation Process\n",
    "    ------------------\n",
    "    For each batch:\n",
    "    1. Forward pass: compute predictions (no gradient computation)\n",
    "    2. Compute loss: compare predictions to targets\n",
    "    3. Track metrics: accumulate loss, predictions, targets, material IDs\n",
    "    \n",
    "    Key Differences from Training\n",
    "    ------------------------------\n",
    "    - model.eval(): Disables dropout, uses running stats for batch norm\n",
    "    - torch.no_grad(): Disables gradient computation (saves memory & time)\n",
    "    - No backpropagation or weight updates\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        CGCNN model to validate\n",
    "    val_loader : DataLoader\n",
    "        DataLoader providing validation/test batches\n",
    "    criterion : nn.Module\n",
    "        Loss function for computing validation loss\n",
    "    device : torch.device\n",
    "        Device to run computations on (CPU or CUDA)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple : (avg_loss, mae, rmse, r2, predictions, targets, cif_ids)\n",
    "        avg_loss : float\n",
    "            Average loss over all validation samples\n",
    "        mae : float\n",
    "            Mean Absolute Error on validation set\n",
    "        rmse : float\n",
    "            Root Mean Squared Error on validation set\n",
    "        r2 : float\n",
    "            R² score (coefficient of determination)\n",
    "            1.0 = perfect fit, 0.0 = no better than mean baseline\n",
    "        predictions : list of float\n",
    "            All predicted values\n",
    "        targets : list of float\n",
    "            All true values\n",
    "        cif_ids : list of str\n",
    "            Material identifiers for each prediction\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - No gradient computation → faster and less memory\n",
    "    - Returns material IDs for error analysis\n",
    "    - Computes R² score in addition to MAE and RMSE\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> val_loss, val_mae, val_rmse, val_r2, preds, targs, ids = validate_epoch(\n",
    "    ...     model, val_loader, criterion, device\n",
    "    ... )\n",
    "    >>> print(f\"Validation - Loss: {val_loss:.4f}, R²: {val_r2:.4f}\")\n",
    "    >>> # Find worst predictions\n",
    "    >>> errors = np.abs(np.array(targs) - np.array(preds))\n",
    "    >>> worst_idx = np.argmax(errors)\n",
    "    >>> print(f\"Worst prediction: {ids[worst_idx]}\")\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    predictions, targets, cif_ids = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation', leave=False)\n",
    "        for batch_idx, (input_data, target, batch_cif_ids) in enumerate(pbar):\n",
    "            atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx = input_data\n",
    "            atom_fea = atom_fea.to(device)\n",
    "            nbr_fea = nbr_fea.to(device)\n",
    "            nbr_fea_idx = nbr_fea_idx.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = model(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            total_loss += loss.item() * target.size(0)\n",
    "            predictions.extend(output.detach().cpu().numpy().flatten())\n",
    "            targets.extend(target.detach().cpu().numpy().flatten())\n",
    "            cif_ids.extend(batch_cif_ids)\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(targets, predictions))\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    return avg_loss, mae, rmse, r2, predictions, targets, cif_ids\n",
    "\n",
    "\n",
    "print(\"✅ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb4d538",
   "metadata": {},
   "source": [
    "## Part 8: Training Loop\n",
    "\n",
    "### Theoretical Concepts\n",
    "\n",
    "#### Training Strategy\n",
    "\n",
    "The training loop implements several best practices for deep learning:\n",
    "\n",
    "1. **Early Stopping**\n",
    "   - Monitors validation loss to prevent overfitting\n",
    "   - Stops training if validation loss doesn't improve for N epochs\n",
    "   - Restores best model weights after training\n",
    "   \n",
    "2. **Learning Rate Scheduling**\n",
    "   - **ReduceLROnPlateau**: Reduces learning rate when validation loss plateaus\n",
    "   - Formula: $\\alpha_{\\text{new}} = \\alpha_{\\text{old}} \\times \\text{factor}$\n",
    "   - Helps model converge to better local minima\n",
    "   \n",
    "3. **Model Checkpointing**\n",
    "   - Saves best model based on validation performance\n",
    "   - Prevents loss of progress if training crashes\n",
    "   - Enables resuming training from best checkpoint\n",
    "\n",
    "#### Overfitting vs Underfitting\n",
    "\n",
    "```\n",
    "Training Loss    Validation Loss    Diagnosis\n",
    "─────────────    ───────────────    ─────────\n",
    "High             High               Underfitting → Increase model capacity\n",
    "Low              High               Overfitting → Regularization needed\n",
    "Low              Low                Good fit → Ready for testing\n",
    "```\n",
    "\n",
    "#### Key Metrics Tracked\n",
    "\n",
    "- **Training Loss**: How well model fits training data\n",
    "- **Validation Loss**: How well model generalizes to unseen data\n",
    "- **MAE/RMSE**: Interpretable error in original units\n",
    "- **R² Score**: Proportion of variance explained by model\n",
    "- **Learning Rate**: Current optimization step size\n",
    "\n",
    "#### Expected Training Behavior\n",
    "\n",
    "1. **Initial Epochs**: Both losses decrease rapidly\n",
    "2. **Middle Epochs**: Slower improvement, validation may fluctuate\n",
    "3. **Late Epochs**: Validation loss plateaus or increases (overfitting)\n",
    "4. **Convergence**: Early stopping triggers, best model restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e3466e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting Training\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                           | 0/9069 [00:00<?, ?it/s]C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 4 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "Training:   0%|                                                   | 2/9069 [00:01<2:27:07,  1.03it/s, loss=1.28e+4]C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 8 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 10 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "Training:   0%|                                                      | 5/9069 [00:05<2:40:49,  1.06s/it, loss=4.37]C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 1 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "Training:   0%|                                                     | 10/9069 [00:09<2:07:38,  1.18it/s, loss=5.08]C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 2 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "Training:   0%|                                                     | 11/9069 [00:10<2:07:33,  1.18it/s, loss=4.98]C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 20 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "Training:   0%|                                                     | 14/9069 [00:13<2:08:43,  1.17it/s, loss=4.45]C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 80 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "Training:   0%|                                                     | 20/9069 [00:18<2:09:07,  1.17it/s, loss=7.38]C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 24 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "Training:   0%|                                                     | 21/9069 [00:20<2:44:06,  1.09s/it, loss=8.12]C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 6 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "Training:   0%|▏                                                    | 29/9069 [00:28<2:45:20,  1.10s/it, loss=6.49]C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 11 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "Training:   0%|▏                                                     | 31/9069 [00:30<2:29:19,  1.01it/s, loss=3.2]C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 18 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "Training:   0%|▏                                                    | 39/9069 [00:39<2:18:14,  1.09it/s, loss=3.97]C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 3 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "Training:   1%|▍                                                    | 71/9069 [01:07<2:13:33,  1.12it/s, loss=2.85]C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 44 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "Training:   1%|▍                                                    | 73/9069 [01:09<1:56:34,  1.29it/s, loss=1.63]C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 28 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "Training:   1%|▌                                                   | 102/9069 [01:37<2:06:57,  1.18it/s, loss=1.38]C:\\Users\\abhin\\Desktop\\cgcnn\\.venv\\Lib\\site-packages\\pymatgen\\core\\structure.py:3109: UserWarning: Issues encountered while parsing CIF: 60 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n",
      "Training:   1%|▋                                                   | 113/9069 [01:47<2:01:40,  1.23it/s, loss=1.59]"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "LEARNING_RATE = 0.1\n",
    "WEIGHT_DECAY = 0.0\n",
    "EPOCHS = 5\n",
    "EARLY_STOPPING_PATIENCE = 30\n",
    "\n",
    "# Initialize tracking\n",
    "history = {\n",
    "    'train_loss': [], 'train_mae': [], 'train_rmse': [],\n",
    "    'val_loss': [], 'val_mae': [], 'val_rmse': [], 'val_r2': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_mae, train_rmse, _, _ = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_mae, val_rmse, val_r2, val_preds, val_targets, _ = validate_epoch(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_mae'].append(train_mae)\n",
    "    history['train_rmse'].append(train_rmse)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_mae'].append(val_mae)\n",
    "    history['val_rmse'].append(val_rmse)\n",
    "    history['val_r2'].append(val_r2)\n",
    "    history['learning_rate'].append(current_lr)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train - Loss: {train_loss:.6f}, MAE: {train_mae:.6f}, RMSE: {train_rmse:.6f}\")\n",
    "    print(f\"Val   - Loss: {val_loss:.6f}, MAE: {val_mae:.6f}, RMSE: {val_rmse:.6f}, R²: {val_r2:.4f}\")\n",
    "    print(f\"LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "        print(f\"✅ New best model! Val Loss: {best_val_loss:.6f}\")\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_mae': val_mae,\n",
    "        }, os.path.join(OUTPUT_DIR, 'best_model.pth'))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"\\n⚠️ Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "    \n",
    "    # Save progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        pd.DataFrame(history).to_csv(os.path.join(OUTPUT_DIR, 'training_history.csv'), index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"✅ Best model loaded\")\n",
    "\n",
    "# Save final history\n",
    "pd.DataFrame(history).to_csv(os.path.join(OUTPUT_DIR, 'training_history.csv'), index=False)\n",
    "print(f\"✅ Training history saved to '{OUTPUT_DIR}/training_history.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea10969",
   "metadata": {},
   "source": [
    "## Part 9: Visualize Training Progress\n",
    "\n",
    "### Purpose of Visualization\n",
    "\n",
    "Training curves help diagnose model behavior and identify issues:\n",
    "\n",
    "#### 1. Loss Curves (Top Left)\n",
    "- **Both decreasing**: Model is learning\n",
    "- **Training low, validation high**: Overfitting\n",
    "- **Both high**: Underfitting\n",
    "- **Oscillating**: Learning rate too high or batch size too small\n",
    "\n",
    "#### 2. MAE Curves (Top Right)\n",
    "- More interpretable than MSE (same units as target)\n",
    "- Should follow similar pattern to loss curves\n",
    "- Useful for comparing model versions\n",
    "\n",
    "#### 3. RMSE Curves (Bottom Left)\n",
    "- Penalizes outliers more than MAE\n",
    "- Higher RMSE relative to MAE → presence of large errors\n",
    "- RMSE ≈ MAE → errors are uniform\n",
    "\n",
    "#### 4. R² and Learning Rate (Bottom Right)\n",
    "- **R² approaching 1.0**: Model explains most variance\n",
    "- **R² < 0**: Model worse than predicting mean\n",
    "- **LR decreases**: Scheduler reducing step size for fine-tuning\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "✅ **Good Training**:\n",
    "- Smooth decrease in both train and val losses\n",
    "- Small gap between train and val metrics\n",
    "- R² increasing towards 1.0\n",
    "- LR decreases when validation plateaus\n",
    "\n",
    "⚠️ **Potential Issues**:\n",
    "- Large train/val gap → Overfitting (add regularization)\n",
    "- Oscillating curves → Reduce learning rate\n",
    "- Val loss increasing → Early stopping needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(epochs_range, history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "axes[0, 0].plot(epochs_range, history['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('MSE Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[0, 1].plot(epochs_range, history['train_mae'], 'b-', label='Train', linewidth=2)\n",
    "axes[0, 1].plot(epochs_range, history['val_mae'], 'r-', label='Validation', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('MAE')\n",
    "axes[0, 1].set_title('Mean Absolute Error', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "axes[1, 0].plot(epochs_range, history['train_rmse'], 'b-', label='Train', linewidth=2)\n",
    "axes[1, 0].plot(epochs_range, history['val_rmse'], 'r-', label='Validation', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('RMSE')\n",
    "axes[1, 0].set_title('Root Mean Squared Error', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# R² and LR\n",
    "ax1 = axes[1, 1]\n",
    "ax2 = ax1.twinx()\n",
    "line1 = ax1.plot(epochs_range, history['val_r2'], 'g-', label='R²', linewidth=2)\n",
    "line2 = ax2.plot(epochs_range, history['learning_rate'], 'orange', label='LR', linewidth=2, linestyle='--')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('R² Score', color='g')\n",
    "ax2.set_ylabel('Learning Rate', color='orange')\n",
    "ax1.set_title('R² Score and Learning Rate', fontweight='bold')\n",
    "ax1.tick_params(axis='y', labelcolor='g')\n",
    "ax2.tick_params(axis='y', labelcolor='orange')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'training_progress.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ Saved to '{OUTPUT_DIR}/training_progress.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e58845e",
   "metadata": {},
   "source": [
    "## Part 10: Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Evaluating on Test Set\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_loss, test_mae, test_rmse, test_r2, test_preds, test_targets, test_cif_ids = validate_epoch(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Test Set Performance:\")\n",
    "print(f\"   Loss (MSE): {test_loss:.6f}\")\n",
    "print(f\"   MAE:        {test_mae:.6f}\")\n",
    "print(f\"   RMSE:       {test_rmse:.6f}\")\n",
    "print(f\"   R² Score:   {test_r2:.4f}\")\n",
    "\n",
    "# Save results\n",
    "test_results = pd.DataFrame({\n",
    "    'material_id': test_cif_ids,\n",
    "    'true_value': test_targets,\n",
    "    'predicted_value': test_preds,\n",
    "    'error': np.array(test_targets) - np.array(test_preds),\n",
    "    'abs_error': np.abs(np.array(test_targets) - np.array(test_preds))\n",
    "})\n",
    "\n",
    "test_results_sorted = test_results.sort_values('abs_error', ascending=False)\n",
    "test_results.to_csv(os.path.join(OUTPUT_DIR, 'test_results.csv'), index=False)\n",
    "\n",
    "print(f\"\\n📋 Top 5 Best Predictions:\")\n",
    "print(test_results_sorted.tail(5)[['material_id', 'true_value', 'predicted_value', 'abs_error']])\n",
    "\n",
    "print(f\"\\n⚠️ Top 5 Worst Predictions:\")\n",
    "print(test_results_sorted.head(5)[['material_id', 'true_value', 'predicted_value', 'abs_error']])\n",
    "\n",
    "print(f\"\\n✅ Test results saved to '{OUTPUT_DIR}/test_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542a399",
   "metadata": {},
   "source": [
    "## Part 11: Prediction Analysis Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91117bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Predicted vs True\n",
    "axes[0, 0].scatter(test_targets, test_preds, alpha=0.6, s=100, edgecolors='black', linewidth=0.5)\n",
    "axes[0, 0].plot([min(test_targets), max(test_targets)], [min(test_targets), max(test_targets)], \n",
    "                'r--', linewidth=2, label='Perfect')\n",
    "axes[0, 0].set_xlabel('True Values')\n",
    "axes[0, 0].set_ylabel('Predicted Values')\n",
    "axes[0, 0].set_title(f'Predicted vs True\\nR² = {test_r2:.4f}, MAE = {test_mae:.4f}', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = np.array(test_targets) - np.array(test_preds)\n",
    "axes[0, 1].scatter(test_preds, residuals, alpha=0.6, s=100, edgecolors='black', linewidth=0.5)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Predicted Values')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Residual Plot', fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution\n",
    "axes[1, 0].hist(residuals, bins=20, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[1, 0].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Residuals')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title(f'Error Distribution\\nMean={np.mean(residuals):.4f}, Std={np.std(residuals):.4f}', \n",
    "                     fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Absolute error\n",
    "abs_errors = np.abs(residuals)\n",
    "axes[1, 1].hist(abs_errors, bins=20, edgecolor='black', alpha=0.7, color='lightcoral')\n",
    "axes[1, 1].axvline(x=test_mae, color='r', linestyle='--', linewidth=2, label=f'MAE={test_mae:.4f}')\n",
    "axes[1, 1].set_xlabel('Absolute Error')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Absolute Error Distribution', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'prediction_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ Saved to '{OUTPUT_DIR}/prediction_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803a576",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Training Complete!**\n",
    "\n",
    "All results have been saved to the `training_results/` directory:\n",
    "- `best_model.pth`: Best model checkpoint\n",
    "- `training_history.csv`: Training metrics over epochs\n",
    "- `test_results.csv`: Detailed test predictions\n",
    "- `performance_summary.csv`: Final metrics summary\n",
    "- `training_progress.png`: Training curves\n",
    "- `prediction_analysis.png`: Prediction visualizations\n",
    "- `performance_comparison.png`: Performance comparison\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "1. Analyze error patterns in worst predictions\n",
    "2. Try hyperparameter tuning\n",
    "3. Experiment with different architectures\n",
    "4. Use the trained model for predictions on new materials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
