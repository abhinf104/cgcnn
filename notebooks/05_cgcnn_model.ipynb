{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24853dcf",
   "metadata": {},
   "source": [
    "# Crystal Graph Convolutional Neural Networks (CGCNN)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook implements the **Crystal Graph Convolutional Neural Network (CGCNN)** architecture, a deep learning framework designed for predicting material properties directly from crystal structures.\n",
    "\n",
    "### Reference\n",
    "**Paper**: Xie, T., & Grossman, J. C. (2018). \"Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties.\" *Physical Review Letters*, 120(14), 145301.\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "### 1. Graph Representation of Crystals\n",
    "\n",
    "In CGCNN, crystal structures are represented as **multigraphs** where:\n",
    "- **Nodes (Vertices)**: Represent atoms in the crystal\n",
    "- **Edges**: Represent bonds/interactions between atoms\n",
    "- **Node Features**: Atom type, electron configuration, electronegativity, etc.\n",
    "- **Edge Features**: Distance, bond type, coordination information\n",
    "\n",
    "### 2. Convolution on Crystal Graphs\n",
    "\n",
    "Unlike standard CNNs that operate on regular grids (images), crystal graphs have:\n",
    "- Variable number of neighbors per atom\n",
    "- Non-Euclidean structure\n",
    "- Periodic boundary conditions\n",
    "\n",
    "The convolutional operation aggregates information from neighboring atoms to update each atom's feature representation.\n",
    "\n",
    "### 3. Message Passing Framework\n",
    "\n",
    "The convolution layer implements a **message passing** scheme:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_i^{(t+1)} = \\mathbf{v}_i^{(t)} + \\sum_{j \\in \\mathcal{N}(i)} \\sigma\\left(\\mathbf{z}_{ij}^{(t)}\\right) \\odot \\mathbf{g}\\left(\\mathbf{z}_{ij}^{(t)}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{v}_i^{(t)}$: Feature vector of atom $i$ at layer $t$\n",
    "- $\\mathcal{N}(i)$: Set of neighboring atoms of atom $i$\n",
    "- $\\mathbf{z}_{ij}^{(t)} = \\mathbf{v}_i^{(t)} \\oplus \\mathbf{v}_j^{(t)} \\oplus \\mathbf{u}_{ij}$: Concatenation of central atom, neighbor, and edge features\n",
    "- $\\sigma$: Sigmoid activation (gate function)\n",
    "- $\\mathbf{g}$: Core message function (typically with softplus activation)\n",
    "- $\\odot$: Element-wise multiplication\n",
    "\n",
    "### 4. Architecture Components\n",
    "\n",
    "#### a) **Embedding Layer**\n",
    "Projects initial atom features to a learnable hidden representation:\n",
    "$$\n",
    "\\mathbf{v}_i^{(0)} = \\mathbf{W}_0 \\mathbf{x}_i + \\mathbf{b}_0\n",
    "$$\n",
    "\n",
    "#### b) **Convolutional Layers**\n",
    "Multiple graph convolution layers that progressively refine atomic representations by aggregating local neighborhood information.\n",
    "\n",
    "#### c) **Pooling Layer**\n",
    "Aggregates all atomic features in a crystal to obtain a crystal-level representation:\n",
    "$$\n",
    "\\mathbf{v}_{\\text{crystal}} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{v}_i^{(T)}\n",
    "$$\n",
    "(Mean pooling over all atoms)\n",
    "\n",
    "#### d) **Fully Connected Layers**\n",
    "Maps the pooled crystal representation to the target property (e.g., formation energy, band gap).\n",
    "\n",
    "### 5. Key Innovations\n",
    "\n",
    "1. **Gating Mechanism**: The sigmoid gate ($\\sigma$) allows the network to learn which neighbor information is relevant\n",
    "2. **Residual Connections**: Skip connections ($\\mathbf{v}_i^{(t)} +$ ...) help with gradient flow\n",
    "3. **Batch Normalization**: Stabilizes training and improves convergence\n",
    "4. **Invariance to Atom Ordering**: Pooling ensures the prediction is invariant to the order of atoms\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c58b47",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61903a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.0+cu126\n",
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d65a896",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Convolutional Layer\n",
    "\n",
    "### Theory: Graph Convolution Operation\n",
    "\n",
    "The `ConvLayer` performs the core graph convolution operation. For each atom, it:\n",
    "\n",
    "1. **Gathers neighbor features**: Retrieves features from all neighboring atoms\n",
    "2. **Concatenates features**: Combines central atom, neighbor, and edge features\n",
    "3. **Applies gating**: Uses a sigmoid gate to control information flow\n",
    "4. **Aggregates**: Sums the gated messages from all neighbors\n",
    "5. **Updates**: Adds the aggregated message to the original features (residual connection)\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given:\n",
    "- $\\mathbf{v}_i \\in \\mathbb{R}^{d}$: Central atom feature (atom_fea_len)\n",
    "- $\\mathbf{v}_j \\in \\mathbb{R}^{d}$: Neighbor atom feature\n",
    "- $\\mathbf{u}_{ij} \\in \\mathbb{R}^{d'}$: Edge feature (nbr_fea_len)\n",
    "\n",
    "The convolution operation:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_{ij} = [\\mathbf{v}_i \\oplus \\mathbf{v}_j \\oplus \\mathbf{u}_{ij}] \\in \\mathbb{R}^{2d + d'}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{f}_{ij}, \\mathbf{c}_{ij} = \\text{split}\\left(\\text{BN}\\left(\\mathbf{W} \\mathbf{z}_{ij} + \\mathbf{b}\\right)\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_{ij} = \\sigma(\\mathbf{f}_{ij}) \\odot \\text{softplus}(\\mathbf{c}_{ij})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_i' = \\text{softplus}\\left(\\mathbf{v}_i + \\text{BN}\\left(\\sum_{j \\in \\mathcal{N}(i)} \\mathbf{m}_{ij}\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "997eec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional operation on graphs\n",
    "    \n",
    "    This layer implements the graph convolution operation that aggregates\n",
    "    information from neighboring atoms using a gating mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, atom_fea_len, nbr_fea_len):\n",
    "        \"\"\"\n",
    "        Initialize ConvLayer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        atom_fea_len: int\n",
    "          Number of atom hidden features.\n",
    "        nbr_fea_len: int\n",
    "          Number of bond features.\n",
    "        \"\"\"\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.atom_fea_len = atom_fea_len\n",
    "        self.nbr_fea_len = nbr_fea_len\n",
    "        \n",
    "        # Fully connected layer: maps concatenated features to gated features\n",
    "        # Input: [atom_i, atom_j, edge_ij] -> Output: [filter, core]\n",
    "        self.fc_full = nn.Linear(2*self.atom_fea_len+self.nbr_fea_len,\n",
    "                                 2*self.atom_fea_len)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.sigmoid = nn.Sigmoid()      # Gate function (0 to 1)\n",
    "        self.softplus1 = nn.Softplus()   # Smooth approximation of ReLU\n",
    "        self.softplus2 = nn.Softplus()\n",
    "        \n",
    "        # Batch normalization layers for training stability\n",
    "        self.bn1 = nn.BatchNorm1d(2*self.atom_fea_len)\n",
    "        self.bn2 = nn.BatchNorm1d(self.atom_fea_len)\n",
    "\n",
    "    def forward(self, atom_in_fea, nbr_fea, nbr_fea_idx):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "\n",
    "        N: Total number of atoms in the batch\n",
    "        M: Max number of neighbors\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        atom_in_fea: Variable(torch.Tensor) shape (N, atom_fea_len)\n",
    "          Atom hidden features before convolution\n",
    "        nbr_fea: Variable(torch.Tensor) shape (N, M, nbr_fea_len)\n",
    "          Bond features of each atom's M neighbors\n",
    "        nbr_fea_idx: torch.LongTensor shape (N, M)\n",
    "          Indices of M neighbors of each atom\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        atom_out_fea: nn.Variable shape (N, atom_fea_len)\n",
    "          Atom hidden features after convolution\n",
    "        \"\"\"\n",
    "        N, M = nbr_fea_idx.shape\n",
    "        \n",
    "        # Step 1: Gather neighbor features using indices\n",
    "        # atom_nbr_fea: (N, M, atom_fea_len)\n",
    "        atom_nbr_fea = atom_in_fea[nbr_fea_idx, :]\n",
    "        \n",
    "        # Step 2: Concatenate [central_atom, neighbor_atom, edge] features\n",
    "        # total_nbr_fea: (N, M, 2*atom_fea_len + nbr_fea_len)\n",
    "        total_nbr_fea = torch.cat(\n",
    "            [atom_in_fea.unsqueeze(1).expand(N, M, self.atom_fea_len),\n",
    "             atom_nbr_fea, nbr_fea], dim=2)\n",
    "        \n",
    "        # Step 3: Apply linear transformation and batch normalization\n",
    "        total_gated_fea = self.fc_full(total_nbr_fea)\n",
    "        total_gated_fea = self.bn1(total_gated_fea.view(\n",
    "            -1, self.atom_fea_len*2)).view(N, M, self.atom_fea_len*2)\n",
    "        \n",
    "        # Step 4: Split into filter (gate) and core (message) components\n",
    "        nbr_filter, nbr_core = total_gated_fea.chunk(2, dim=2)\n",
    "        \n",
    "        # Step 5: Apply activations\n",
    "        nbr_filter = self.sigmoid(nbr_filter)     # Gate: controls information flow\n",
    "        nbr_core = self.softplus1(nbr_core)       # Message: actual information\n",
    "        \n",
    "        # Step 6: Element-wise gating and sum over neighbors\n",
    "        nbr_sumed = torch.sum(nbr_filter * nbr_core, dim=1)\n",
    "        nbr_sumed = self.bn2(nbr_sumed)\n",
    "        \n",
    "        # Step 7: Residual connection + activation\n",
    "        out = self.softplus2(atom_in_fea + nbr_sumed)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1ef27e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete CGCNN Architecture\n",
    "\n",
    "### Theory: End-to-End Network\n",
    "\n",
    "The complete CGCNN architecture consists of:\n",
    "\n",
    "1. **Embedding Layer**: Projects raw atom features to learnable representations\n",
    "2. **Stacked Convolution Layers**: Multiple graph convolutions to capture multi-hop neighborhood information\n",
    "3. **Pooling Layer**: Aggregates atom-level features to crystal-level representation\n",
    "4. **Fully Connected Layers**: Maps crystal features to target properties\n",
    "5. **Output Layer**: \n",
    "   - Regression: Single value (e.g., formation energy)\n",
    "   - Classification: Log-softmax over classes (e.g., metal/non-metal)\n",
    "\n",
    "### Network Flow\n",
    "\n",
    "```\n",
    "Input: (atom_features, edge_features, edge_indices, crystal_mapping)\n",
    "   |\n",
    "   v\n",
    "[Embedding Layer] ‚îÄ‚îÄ> atom_fea_len dimensions\n",
    "   |\n",
    "   v\n",
    "[Conv Layer 1] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Message passing iteration 1\n",
    "   |\n",
    "   v\n",
    "[Conv Layer 2] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Message passing iteration 2\n",
    "   |\n",
    "   v\n",
    "   ...\n",
    "   |\n",
    "   v\n",
    "[Conv Layer n] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Message passing iteration n\n",
    "   |\n",
    "   v\n",
    "[Pooling] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Mean over all atoms per crystal\n",
    "   |\n",
    "   v\n",
    "[FC Layer 1] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> h_fea_len dimensions\n",
    "   |\n",
    "   v\n",
    "[FC Layer 2] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Optional additional hidden layers\n",
    "   |\n",
    "   v\n",
    "[Output Layer] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Prediction (1 for regression, 2+ for classification)\n",
    "```\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "- `orig_atom_fea_len`: Dimension of input atom features (e.g., 92 for one-hot encoding)\n",
    "- `nbr_fea_len`: Dimension of edge features (e.g., distance-based Gaussian expansion)\n",
    "- `atom_fea_len`: Hidden atom feature dimension (default: 64)\n",
    "- `n_conv`: Number of convolution layers (default: 3)\n",
    "- `h_fea_len`: Hidden dimension after pooling (default: 128)\n",
    "- `n_h`: Number of fully connected layers (default: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56aed9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrystalGraphConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Create a crystal graph convolutional neural network for predicting total\n",
    "    material properties.\n",
    "    \"\"\"\n",
    "    def __init__(self, orig_atom_fea_len, nbr_fea_len,\n",
    "                 atom_fea_len=64, n_conv=3, h_fea_len=128, n_h=1,\n",
    "                 classification=False):\n",
    "        \"\"\"\n",
    "        Initialize CrystalGraphConvNet.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        orig_atom_fea_len: int\n",
    "          Number of atom features in the input.\n",
    "        nbr_fea_len: int\n",
    "          Number of bond features.\n",
    "        atom_fea_len: int\n",
    "          Number of hidden atom features in the convolutional layers\n",
    "        n_conv: int\n",
    "          Number of convolutional layers\n",
    "        h_fea_len: int\n",
    "          Number of hidden features after pooling\n",
    "        n_h: int\n",
    "          Number of hidden layers after pooling\n",
    "        classification: bool\n",
    "          Whether this is a classification task\n",
    "        \"\"\"\n",
    "        super(CrystalGraphConvNet, self).__init__()\n",
    "        self.classification = classification\n",
    "        \n",
    "        # Embedding layer: projects input atom features to hidden space\n",
    "        self.embedding = nn.Linear(orig_atom_fea_len, atom_fea_len)\n",
    "        \n",
    "        # Stack of convolutional layers\n",
    "        self.convs = nn.ModuleList([ConvLayer(atom_fea_len=atom_fea_len,\n",
    "                                    nbr_fea_len=nbr_fea_len)\n",
    "                                    for _ in range(n_conv)])\n",
    "        \n",
    "        # Transition from convolutional to fully connected layers\n",
    "        self.conv_to_fc = nn.Linear(atom_fea_len, h_fea_len)\n",
    "        self.conv_to_fc_softplus = nn.Softplus()\n",
    "        \n",
    "        # Additional fully connected hidden layers\n",
    "        if n_h > 1:\n",
    "            self.fcs = nn.ModuleList([nn.Linear(h_fea_len, h_fea_len)\n",
    "                                      for _ in range(n_h-1)])\n",
    "            self.softpluses = nn.ModuleList([nn.Softplus()\n",
    "                                             for _ in range(n_h-1)])\n",
    "        \n",
    "        # Output layer\n",
    "        if self.classification:\n",
    "            self.fc_out = nn.Linear(h_fea_len, 2)  # Binary classification\n",
    "        else:\n",
    "            self.fc_out = nn.Linear(h_fea_len, 1)  # Regression\n",
    "        \n",
    "        # Classification-specific components\n",
    "        if self.classification:\n",
    "            self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "            self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "\n",
    "        N: Total number of atoms in the batch\n",
    "        M: Max number of neighbors\n",
    "        N0: Total number of crystals in the batch\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        atom_fea: Variable(torch.Tensor) shape (N, orig_atom_fea_len)\n",
    "          Atom features from atom type\n",
    "        nbr_fea: Variable(torch.Tensor) shape (N, M, nbr_fea_len)\n",
    "          Bond features of each atom's M neighbors\n",
    "        nbr_fea_idx: torch.LongTensor shape (N, M)\n",
    "          Indices of M neighbors of each atom\n",
    "        crystal_atom_idx: list of torch.LongTensor of length N0\n",
    "          Mapping from the crystal idx to atom idx\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        prediction: nn.Variable shape (N0, ) for regression or (N0, 2) for classification\n",
    "          Predicted property for each crystal\n",
    "        \"\"\"\n",
    "        # Step 1: Embed atom features\n",
    "        atom_fea = self.embedding(atom_fea)\n",
    "        \n",
    "        # Step 2: Apply graph convolutions\n",
    "        for conv_func in self.convs:\n",
    "            atom_fea = conv_func(atom_fea, nbr_fea, nbr_fea_idx)\n",
    "        \n",
    "        # Step 3: Pool atom features to get crystal-level representation\n",
    "        crys_fea = self.pooling(atom_fea, crystal_atom_idx)\n",
    "        \n",
    "        # Step 4: Apply fully connected layers\n",
    "        crys_fea = self.conv_to_fc(self.conv_to_fc_softplus(crys_fea))\n",
    "        crys_fea = self.conv_to_fc_softplus(crys_fea)\n",
    "        \n",
    "        # Apply dropout for classification\n",
    "        if self.classification:\n",
    "            crys_fea = self.dropout(crys_fea)\n",
    "        \n",
    "        # Additional hidden layers if specified\n",
    "        if hasattr(self, 'fcs') and hasattr(self, 'softpluses'):\n",
    "            for fc, softplus in zip(self.fcs, self.softpluses):\n",
    "                crys_fea = softplus(fc(crys_fea))\n",
    "        \n",
    "        # Step 5: Output layer\n",
    "        out = self.fc_out(crys_fea)\n",
    "        \n",
    "        # Apply log-softmax for classification\n",
    "        if self.classification:\n",
    "            out = self.logsoftmax(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def pooling(self, atom_fea, crystal_atom_idx):\n",
    "        \"\"\"\n",
    "        Pooling the atom features to crystal features\n",
    "\n",
    "        N: Total number of atoms in the batch\n",
    "        N0: Total number of crystals in the batch\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        atom_fea: Variable(torch.Tensor) shape (N, atom_fea_len)\n",
    "          Atom feature vectors of the batch\n",
    "        crystal_atom_idx: list of torch.LongTensor of length N0\n",
    "          Mapping from the crystal idx to atom idx\n",
    "          \n",
    "        Returns\n",
    "        -------\n",
    "        crys_fea: Variable(torch.Tensor) shape (N0, atom_fea_len)\n",
    "          Crystal feature vectors\n",
    "        \"\"\"\n",
    "        # Verify that all atoms are accounted for\n",
    "        assert sum([len(idx_map) for idx_map in crystal_atom_idx]) == \\\n",
    "            atom_fea.data.shape[0]\n",
    "        \n",
    "        # Average pooling: mean of all atom features in each crystal\n",
    "        summed_fea = [torch.mean(atom_fea[idx_map], dim=0, keepdim=True)\n",
    "                      for idx_map in crystal_atom_idx]\n",
    "        \n",
    "        return torch.cat(summed_fea, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7bf6a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Instantiation and Summary\n",
    "\n",
    "Let's create example models for both regression and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f69d527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REGRESSION MODEL: Predicting Formation Energy\n",
      "================================================================================\n",
      "CrystalGraphConvNet(\n",
      "  (embedding): Linear(in_features=92, out_features=64, bias=True)\n",
      "  (convs): ModuleList(\n",
      "    (0-2): 3 x ConvLayer(\n",
      "      (fc_full): Linear(in_features=169, out_features=128, bias=True)\n",
      "      (sigmoid): Sigmoid()\n",
      "      (softplus1): Softplus(beta=1.0, threshold=20.0)\n",
      "      (softplus2): Softplus(beta=1.0, threshold=20.0)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv_to_fc): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (conv_to_fc_softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (fc_out): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 80,833\n",
      "Trainable parameters: 80,833\n"
     ]
    }
   ],
   "source": [
    "# Example configuration for regression (e.g., formation energy prediction)\n",
    "print(\"=\" * 80)\n",
    "print(\"REGRESSION MODEL: Predicting Formation Energy\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model_regression = CrystalGraphConvNet(\n",
    "    orig_atom_fea_len=92,    # Atom feature dimension (e.g., from atom_init.json)\n",
    "    nbr_fea_len=41,          # Edge feature dimension (e.g., Gaussian expansion)\n",
    "    atom_fea_len=64,         # Hidden atom features\n",
    "    n_conv=3,                # 3 graph convolution layers\n",
    "    h_fea_len=128,           # Hidden features after pooling\n",
    "    n_h=1,                   # 1 hidden layer after pooling\n",
    "    classification=False     # Regression task\n",
    ")\n",
    "\n",
    "print(model_regression)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model_regression.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model_regression.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d9d1647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLASSIFICATION MODEL: Metal vs. Non-Metal Classification\n",
      "================================================================================\n",
      "CrystalGraphConvNet(\n",
      "  (embedding): Linear(in_features=92, out_features=64, bias=True)\n",
      "  (convs): ModuleList(\n",
      "    (0-2): 3 x ConvLayer(\n",
      "      (fc_full): Linear(in_features=169, out_features=128, bias=True)\n",
      "      (sigmoid): Sigmoid()\n",
      "      (softplus1): Softplus(beta=1.0, threshold=20.0)\n",
      "      (softplus2): Softplus(beta=1.0, threshold=20.0)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv_to_fc): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (conv_to_fc_softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (fc_out): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (logsoftmax): LogSoftmax(dim=1)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      "Total parameters: 80,962\n",
      "Trainable parameters: 80,962\n"
     ]
    }
   ],
   "source": [
    "# Example configuration for classification (e.g., metal vs. non-metal)\n",
    "print(\"=\" * 80)\n",
    "print(\"CLASSIFICATION MODEL: Metal vs. Non-Metal Classification\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model_classification = CrystalGraphConvNet(\n",
    "    orig_atom_fea_len=92,\n",
    "    nbr_fea_len=41,\n",
    "    atom_fea_len=64,\n",
    "    n_conv=3,\n",
    "    h_fea_len=128,\n",
    "    n_h=1,\n",
    "    classification=True      # Classification task\n",
    ")\n",
    "\n",
    "print(model_classification)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model_classification.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model_classification.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0ce2c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Testing with Dummy Data\n",
    "\n",
    "Let's verify the model works correctly with synthetic input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11f58981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes:\n",
      "  atom_fea: torch.Size([14, 92])\n",
      "  nbr_fea: torch.Size([14, 12, 41])\n",
      "  nbr_fea_idx: torch.Size([14, 12])\n",
      "  crystal_atom_idx: 2 crystals\n",
      "    Crystal 0: 8 atoms\n",
      "    Crystal 1: 6 atoms\n"
     ]
    }
   ],
   "source": [
    "# Create dummy data for testing\n",
    "batch_size = 2           # Number of crystals in batch\n",
    "n_atoms_1 = 8            # Number of atoms in crystal 1\n",
    "n_atoms_2 = 6            # Number of atoms in crystal 2\n",
    "total_atoms = n_atoms_1 + n_atoms_2\n",
    "max_neighbors = 12       # Maximum number of neighbors per atom\n",
    "\n",
    "# Atom features: (total_atoms, orig_atom_fea_len)\n",
    "atom_fea = torch.randn(total_atoms, 92)\n",
    "\n",
    "# Neighbor features (edge features): (total_atoms, max_neighbors, nbr_fea_len)\n",
    "nbr_fea = torch.randn(total_atoms, max_neighbors, 41)\n",
    "\n",
    "# Neighbor indices: (total_atoms, max_neighbors)\n",
    "# Random indices pointing to neighbor atoms\n",
    "nbr_fea_idx = torch.randint(0, total_atoms, (total_atoms, max_neighbors))\n",
    "\n",
    "# Crystal-to-atom mapping: list of tensors indicating which atoms belong to which crystal\n",
    "crystal_atom_idx = [\n",
    "    torch.LongTensor(list(range(0, n_atoms_1))),           # Crystal 1: atoms 0-7\n",
    "    torch.LongTensor(list(range(n_atoms_1, total_atoms)))  # Crystal 2: atoms 8-13\n",
    "]\n",
    "\n",
    "print(\"Input shapes:\")\n",
    "print(f\"  atom_fea: {atom_fea.shape}\")\n",
    "print(f\"  nbr_fea: {nbr_fea.shape}\")\n",
    "print(f\"  nbr_fea_idx: {nbr_fea_idx.shape}\")\n",
    "print(f\"  crystal_atom_idx: {len(crystal_atom_idx)} crystals\")\n",
    "print(f\"    Crystal 0: {len(crystal_atom_idx[0])} atoms\")\n",
    "print(f\"    Crystal 1: {len(crystal_atom_idx[1])} atoms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e02ccc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING REGRESSION MODEL\n",
      "================================================================================\n",
      "\n",
      "Output shape: torch.Size([2, 1])\n",
      "Output values (predicted properties):\n",
      "tensor([[-0.7730],\n",
      "        [-1.0977]])\n",
      "\n",
      "Expected: (2, 1) for regression\n",
      "Actual: torch.Size([2, 1])\n",
      "‚úì Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test regression model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTING REGRESSION MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model_regression.eval()\n",
    "with torch.no_grad():\n",
    "    output_regression = model_regression(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "\n",
    "print(f\"\\nOutput shape: {output_regression.shape}\")\n",
    "print(f\"Output values (predicted properties):\")\n",
    "print(output_regression)\n",
    "print(f\"\\nExpected: ({batch_size}, 1) for regression\")\n",
    "print(f\"Actual: {output_regression.shape}\")\n",
    "print(f\"‚úì Test passed!\" if output_regression.shape == (batch_size, 1) else \"‚úó Test failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "853e259e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING CLASSIFICATION MODEL\n",
      "================================================================================\n",
      "\n",
      "Output shape: torch.Size([2, 2])\n",
      "Output values (log probabilities):\n",
      "tensor([[-1.5023e-03, -6.5015e+00],\n",
      "        [-8.7009e-04, -7.0474e+00]])\n",
      "\n",
      "Probabilities (after exp):\n",
      "tensor([[9.9850e-01, 1.5013e-03],\n",
      "        [9.9913e-01, 8.6966e-04]])\n",
      "\n",
      "Expected: (2, 2) for binary classification\n",
      "Actual: torch.Size([2, 2])\n",
      "‚úì Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test classification model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTING CLASSIFICATION MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model_classification.eval()\n",
    "with torch.no_grad():\n",
    "    output_classification = model_classification(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "\n",
    "print(f\"\\nOutput shape: {output_classification.shape}\")\n",
    "print(f\"Output values (log probabilities):\")\n",
    "print(output_classification)\n",
    "print(f\"\\nProbabilities (after exp):\")\n",
    "print(torch.exp(output_classification))\n",
    "print(f\"\\nExpected: ({batch_size}, 2) for binary classification\")\n",
    "print(f\"Actual: {output_classification.shape}\")\n",
    "print(f\"‚úì Test passed!\" if output_classification.shape == (batch_size, 2) else \"‚úó Test failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e1685",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training Considerations\n",
    "\n",
    "### Loss Functions\n",
    "\n",
    "**Regression:**\n",
    "- Mean Squared Error (MSE): $\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$\n",
    "- Mean Absolute Error (MAE): $\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|$\n",
    "\n",
    "**Classification:**\n",
    "- Negative Log-Likelihood (NLL): $\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p(y_i | \\mathbf{x}_i)$\n",
    "\n",
    "### Optimization\n",
    "\n",
    "- **Optimizer**: Adam, AdamW (commonly used)\n",
    "- **Learning Rate**: 0.001 - 0.01 (with learning rate scheduling)\n",
    "- **Batch Size**: 128 - 512 (depends on GPU memory)\n",
    "- **Epochs**: 100 - 500 (with early stopping)\n",
    "\n",
    "### Data Normalization\n",
    "\n",
    "For regression tasks, normalize target values:\n",
    "$$\n",
    "y_{\\text{norm}} = \\frac{y - \\mu_y}{\\sigma_y}\n",
    "$$\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "**Regression:**\n",
    "- Mean Absolute Error (MAE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- $R^2$ Score\n",
    "\n",
    "**Classification:**\n",
    "- Accuracy\n",
    "- Precision, Recall, F1-Score\n",
    "- ROC-AUC\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b052ec",
   "metadata": {},
   "source": [
    "### Step 1: Import Additional Libraries for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a31f618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import functools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from pymatgen.core.structure import Structure\n",
    "from pymatgen.core.periodic_table import Element\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3b606",
   "metadata": {},
   "source": [
    "### Step 2: Define CGCNN Data Loading Classes\n",
    "\n",
    "These classes are needed to load crystal structures and create graph representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "324df102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CGCNN classes defined: GaussianDistance, AtomInitializer, AtomCustomJSONInitializer\n"
     ]
    }
   ],
   "source": [
    "# Define GaussianDistance class for edge feature expansion\n",
    "class GaussianDistance(object):\n",
    "    \"\"\"Expands the distance by Gaussian basis functions.\"\"\"\n",
    "    def __init__(self, dmin, dmax, step, var=None):\n",
    "        assert dmin < dmax\n",
    "        assert dmax - dmin > step\n",
    "        self.filter = np.arange(dmin, dmax+step, step)\n",
    "        if var is None:\n",
    "            var = step\n",
    "        self.var = var\n",
    "\n",
    "    def expand(self, distances):\n",
    "        \"\"\"Apply Gaussian distance filter to distance array.\"\"\"\n",
    "        return np.exp(-(distances[..., np.newaxis] - self.filter)**2 / self.var**2)\n",
    "\n",
    "\n",
    "# Define AtomInitializer classes\n",
    "class AtomInitializer(object):\n",
    "    \"\"\"Base class for initializing atom feature vectors.\"\"\"\n",
    "    def __init__(self, atom_types):\n",
    "        self.atom_types = set(atom_types)\n",
    "        self._embedding = {}\n",
    "\n",
    "    def get_atom_fea(self, atom_type):\n",
    "        assert atom_type in self.atom_types\n",
    "        return self._embedding[atom_type]\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._embedding = state_dict\n",
    "        self.atom_types = set(self._embedding.keys())\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._embedding\n",
    "\n",
    "\n",
    "class AtomCustomJSONInitializer(AtomInitializer):\n",
    "    \"\"\"Initialize atom features from JSON file mapping atomic numbers to embeddings.\"\"\"\n",
    "    def __init__(self, elem_embedding_file):\n",
    "        from pymatgen.core.periodic_table import Element\n",
    "        \n",
    "        with open(elem_embedding_file) as f:\n",
    "            elem_embedding = json.load(f)\n",
    "        \n",
    "        # Handle both formats: symbol keys and integer keys\n",
    "        converted_embedding = {}\n",
    "        for key, value in elem_embedding.items():\n",
    "            try:\n",
    "                # Try to convert key to integer (if already numeric)\n",
    "                atomic_num = int(key)\n",
    "            except ValueError:\n",
    "                # If key is a symbol, convert to atomic number\n",
    "                try:\n",
    "                    atomic_num = Element(key).Z\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not process element '{key}': {e}\")\n",
    "                    continue\n",
    "            \n",
    "            converted_embedding[atomic_num] = value\n",
    "        \n",
    "        atom_types = set(converted_embedding.keys())\n",
    "        super(AtomCustomJSONInitializer, self).__init__(atom_types)\n",
    "        \n",
    "        for key, value in converted_embedding.items():\n",
    "            self._embedding[key] = np.array(value, dtype=float)\n",
    "\n",
    "\n",
    "print(\"‚úÖ CGCNN classes defined: GaussianDistance, AtomInitializer, AtomCustomJSONInitializer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e42ed",
   "metadata": {},
   "source": [
    "### Step 3: Define CIFData Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2aa8a73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CIFData class defined\n"
     ]
    }
   ],
   "source": [
    "# Define CIFData dataset class\n",
    "class CIFData(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for crystal structures stored as CIF files.\n",
    "    \n",
    "    Expected directory structure:\n",
    "        root_dir/\n",
    "        ‚îú‚îÄ‚îÄ id_prop.csv          # material_id, target_property\n",
    "        ‚îú‚îÄ‚îÄ atom_init.json       # atom embeddings\n",
    "        ‚îú‚îÄ‚îÄ material1.cif\n",
    "        ‚îú‚îÄ‚îÄ material2.cif\n",
    "        ‚îî‚îÄ‚îÄ ...\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, max_num_nbr=12, radius=8, dmin=0, step=0.2,\n",
    "                 random_seed=123):\n",
    "        self.root_dir = root_dir\n",
    "        self.max_num_nbr, self.radius = max_num_nbr, radius\n",
    "        \n",
    "        assert os.path.exists(root_dir), f'root_dir does not exist: {root_dir}'\n",
    "        \n",
    "        # Load id_prop.csv\n",
    "        id_prop_file = os.path.join(self.root_dir, 'id_prop.csv')\n",
    "        assert os.path.exists(id_prop_file), f'id_prop.csv not found: {id_prop_file}'\n",
    "        \n",
    "        with open(id_prop_file) as f:\n",
    "            reader = csv.reader(f)\n",
    "            self.id_prop_data = [row for row in reader]\n",
    "        \n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(self.id_prop_data)\n",
    "        \n",
    "        # Load atom embeddings\n",
    "        atom_init_file = os.path.join(self.root_dir, 'atom_init.json')\n",
    "        assert os.path.exists(atom_init_file), f'atom_init.json not found: {atom_init_file}'\n",
    "        \n",
    "        self.ari = AtomCustomJSONInitializer(atom_init_file)\n",
    "        self.gdf = GaussianDistance(dmin=dmin, dmax=self.radius, step=step)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id_prop_data)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def __getitem__(self, idx):\n",
    "        cif_id, target = self.id_prop_data[idx]\n",
    "        crystal = Structure.from_file(os.path.join(self.root_dir, cif_id+'.cif'))\n",
    "        \n",
    "        # Get atom features\n",
    "        atom_fea = np.vstack([self.ari.get_atom_fea(crystal[i].specie.number)\n",
    "                              for i in range(len(crystal))])\n",
    "        atom_fea = torch.Tensor(atom_fea)\n",
    "        \n",
    "        # Get neighbors within cutoff radius\n",
    "        all_nbrs = crystal.get_all_neighbors(self.radius, include_index=True)\n",
    "        all_nbrs = [sorted(nbrs, key=lambda x: x[1]) for nbrs in all_nbrs]\n",
    "        \n",
    "        # Build neighbor feature and index arrays\n",
    "        nbr_fea_idx, nbr_fea = [], []\n",
    "        for nbr in all_nbrs:\n",
    "            if len(nbr) < self.max_num_nbr:\n",
    "                warnings.warn(f'{cif_id}: not enough neighbors (found {len(nbr)}, need {self.max_num_nbr})')\n",
    "                nbr_fea_idx.append(list(map(lambda x: x[2], nbr)) +\n",
    "                                   [0] * (self.max_num_nbr - len(nbr)))\n",
    "                nbr_fea.append(list(map(lambda x: x[1], nbr)) +\n",
    "                               [self.radius + 1.] * (self.max_num_nbr - len(nbr)))\n",
    "            else:\n",
    "                nbr_fea_idx.append(list(map(lambda x: x[2], nbr[:self.max_num_nbr])))\n",
    "                nbr_fea.append(list(map(lambda x: x[1], nbr[:self.max_num_nbr])))\n",
    "        \n",
    "        nbr_fea_idx, nbr_fea = np.array(nbr_fea_idx), np.array(nbr_fea)\n",
    "        nbr_fea = self.gdf.expand(nbr_fea)\n",
    "        \n",
    "        nbr_fea = torch.Tensor(nbr_fea)\n",
    "        nbr_fea_idx = torch.LongTensor(nbr_fea_idx)\n",
    "        target = torch.Tensor([float(target)])\n",
    "        \n",
    "        return (atom_fea, nbr_fea, nbr_fea_idx), target, cif_id\n",
    "\n",
    "\n",
    "print(\"‚úÖ CIFData class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f900d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ collate_pool and get_train_val_test_loader functions defined\n"
     ]
    }
   ],
   "source": [
    "# Define collate function for batching variable-sized graphs\n",
    "def collate_pool(dataset_list):\n",
    "    \"\"\"\n",
    "    Collate a list of data and return a batch for predicting crystal properties.\n",
    "    Handles variable-sized graphs by concatenating them into a single large graph.\n",
    "    \"\"\"\n",
    "    batch_atom_fea, batch_nbr_fea, batch_nbr_fea_idx = [], [], []\n",
    "    crystal_atom_idx, batch_target = [], []\n",
    "    batch_cif_ids = []\n",
    "    base_idx = 0\n",
    "    \n",
    "    for i, ((atom_fea, nbr_fea, nbr_fea_idx), target, cif_id) in enumerate(dataset_list):\n",
    "        n_i = atom_fea.shape[0]  # number of atoms for this crystal\n",
    "        batch_atom_fea.append(atom_fea)\n",
    "        batch_nbr_fea.append(nbr_fea)\n",
    "        batch_nbr_fea_idx.append(nbr_fea_idx + base_idx)\n",
    "        \n",
    "        new_idx = torch.LongTensor(np.arange(n_i) + base_idx)\n",
    "        crystal_atom_idx.append(new_idx)\n",
    "        batch_target.append(target)\n",
    "        batch_cif_ids.append(cif_id)\n",
    "        base_idx += n_i\n",
    "    \n",
    "    return (torch.cat(batch_atom_fea, dim=0),\n",
    "            torch.cat(batch_nbr_fea, dim=0),\n",
    "            torch.cat(batch_nbr_fea_idx, dim=0),\n",
    "            crystal_atom_idx), \\\n",
    "           torch.stack(batch_target, dim=0), \\\n",
    "           batch_cif_ids\n",
    "\n",
    "\n",
    "# Define data loader creation function\n",
    "def get_train_val_test_loader(dataset, collate_fn=default_collate,\n",
    "                              batch_size=64, train_ratio=None,\n",
    "                              val_ratio=0.1, test_ratio=0.1, return_test=False,\n",
    "                              num_workers=0, pin_memory=False, **kwargs):\n",
    "    \"\"\"Create train, validation, and test data loaders with proper splitting.\"\"\"\n",
    "    \n",
    "    total_size = len(dataset)\n",
    "    if train_ratio is None:\n",
    "        assert val_ratio + test_ratio < 1\n",
    "        train_ratio = 1 - val_ratio - test_ratio\n",
    "        print(f'[Info] train_ratio not specified, using {train_ratio} for training')\n",
    "    else:\n",
    "        assert train_ratio + val_ratio + test_ratio <= 1\n",
    "    \n",
    "    indices = list(range(total_size))\n",
    "    \n",
    "    train_size = kwargs.get('train_size', int(train_ratio * total_size))\n",
    "    test_size = kwargs.get('test_size', int(test_ratio * total_size))\n",
    "    valid_size = kwargs.get('val_size', int(val_ratio * total_size))\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(indices[:train_size])\n",
    "    val_sampler = SubsetRandomSampler(indices[-(valid_size + test_size):-test_size])\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                              sampler=train_sampler,\n",
    "                              num_workers=num_workers,\n",
    "                              collate_fn=collate_fn, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                            sampler=val_sampler,\n",
    "                            num_workers=num_workers,\n",
    "                            collate_fn=collate_fn, pin_memory=pin_memory)\n",
    "    \n",
    "    if return_test:\n",
    "        test_sampler = SubsetRandomSampler(indices[-test_size:])\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                                 sampler=test_sampler,\n",
    "                                 num_workers=num_workers,\n",
    "                                 collate_fn=collate_fn, pin_memory=pin_memory)\n",
    "        return train_loader, val_loader, test_loader\n",
    "    else:\n",
    "        return train_loader, val_loader\n",
    "\n",
    "\n",
    "print(\"‚úÖ collate_pool and get_train_val_test_loader functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6904e5",
   "metadata": {},
   "source": [
    "### Step 4: Load Dataset from Notebook 02\n",
    "\n",
    "We'll use the CIF structures created in notebook 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac9241-6938-4c04-a5b1-937ce27cd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = CIFData(\n",
    "    root_dir=CIF_DIR,\n",
    "    max_num_nbr=MAX_NUM_NBR,\n",
    "    radius=RADIUS,\n",
    "    dmin=DMIN,\n",
    "    step=DSTEP,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Get feature dimensions from first sample\n",
    "(atom_fea, nbr_fea, nbr_fea_idx), target, cif_id = dataset[0]\n",
    "ORIG_ATOM_FEA_LEN = atom_fea.shape[1]\n",
    "NBR_FEA_LEN = nbr_fea.shape[2]\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded:\")\n",
    "print(f\"   Total samples: {len(dataset)}\")\n",
    "print(f\"   Atom feature length: {ORIG_ATOM_FEA_LEN}\")\n",
    "print(f\"   Neighbor feature length: {NBR_FEA_LEN}\")\n",
    "print(f\"   First sample: {cif_id}, target: {target.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a7f72d",
   "metadata": {},
   "source": [
    "### Step 5: Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc476931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error creating data loaders: name 'dataset' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\abhin\\AppData\\Local\\Temp\\ipykernel_948\\1469353720.py\", line 4, in <module>\n",
      "    dataset=dataset,\n",
      "            ^^^^^^^\n",
      "NameError: name 'dataset' is not defined. Did you mean: 'Dataset'?\n"
     ]
    }
   ],
   "source": [
    "# Create train, validation, and test data loaders\n",
    "try:\n",
    "    train_loader, val_loader, test_loader = get_train_val_test_loader(\n",
    "        dataset=dataset,\n",
    "        collate_fn=collate_pool,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.15,\n",
    "        test_ratio=0.15,\n",
    "        return_test=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Data Loaders Created:\")\n",
    "    print(f\"   Training batches: {len(train_loader)}\")\n",
    "    print(f\"   Validation batches: {len(val_loader)}\")\n",
    "    print(f\"   Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Calculate approximate dataset split sizes\n",
    "    train_size = len(train_loader) * BATCH_SIZE\n",
    "    val_size = len(val_loader) * BATCH_SIZE\n",
    "    test_size = len(test_loader) * BATCH_SIZE\n",
    "    \n",
    "    print(f\"\\nüìä Approximate dataset split:\")\n",
    "    print(f\"   Training samples: ~{train_size}\")\n",
    "    print(f\"   Validation samples: ~{val_size}\")\n",
    "    print(f\"   Test samples: ~{test_size}\")\n",
    "    print(f\"   Total: {len(dataset)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating data loaders: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f0ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CGCNN Configuration\n",
    "MAX_NUM_NBR = 12\n",
    "RADIUS = 8.0\n",
    "DMIN = 0\n",
    "DSTEP = 0.2\n",
    "BATCH_SIZE = 32\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Path to the CIF structures directory\n",
    "CIF_DIR = os.path.join('..', 'notebooks', 'cif_structures')\n",
    "\n",
    "print(f\"CGCNN Configuration:\")\n",
    "print(f\"  Max neighbors per atom: {MAX_NUM_NBR}\")\n",
    "print(f\"  Cutoff radius: {RADIUS} √Ö\")\n",
    "print(f\"  Gaussian distance range: {DMIN} to {RADIUS}\")\n",
    "print(f\"  Gaussian distance step: {DSTEP}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Random seed: {RANDOM_SEED}\")\n",
    "print(f\"  CIF directory: {CIF_DIR}\")\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    dataset = CIFData(\n",
    "        root_dir=CIF_DIR,\n",
    "        max_num_nbr=MAX_NUM_NBR,\n",
    "        radius=RADIUS,\n",
    "        dmin=DMIN,\n",
    "        step=DSTEP,\n",
    "        random_seed=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    # Get dimensions from first sample\n",
    "    (atom_fea, nbr_fea, nbr_fea_idx), target, cif_id = dataset[0]\n",
    "    ORIG_ATOM_FEA_LEN = atom_fea.shape[1]\n",
    "    NBR_FEA_LEN = nbr_fea.shape[2]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset loaded successfully:\")\n",
    "    print(f\"   Total samples: {len(dataset)}\")\n",
    "    print(f\"   Original atom feature length: {ORIG_ATOM_FEA_LEN}\")\n",
    "    print(f\"   Neighbor feature length: {NBR_FEA_LEN}\")\n",
    "    print(f\"   First sample: {cif_id}, target: {target.item():.6f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7686c",
   "metadata": {},
   "source": [
    "### Step 6: Initialize Model, Loss Function, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d62b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "ATOM_FEA_LEN = 64\n",
    "N_CONV = 3\n",
    "H_FEA_LEN = 128\n",
    "N_H = 1\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0\n",
    "EPOCHS = 100\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = CrystalGraphConvNet(\n",
    "    orig_atom_fea_len=ORIG_ATOM_FEA_LEN,\n",
    "    nbr_fea_len=NBR_FEA_LEN,\n",
    "    atom_fea_len=ATOM_FEA_LEN,\n",
    "    n_conv=N_CONV,\n",
    "    h_fea_len=H_FEA_LEN,\n",
    "    n_h=N_H,\n",
    "    classification=False  # Regression task\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\n‚úÖ Model initialized:\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Loss function (MSE for regression)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Training setup complete:\")\n",
    "print(f\"   Loss function: MSE\")\n",
    "print(f\"   Optimizer: Adam\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"   Epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144280cf",
   "metadata": {},
   "source": [
    "### Step 7: Define Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c979fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training', leave=False)\n",
    "    for batch_idx, (input_data, target, _) in enumerate(pbar):\n",
    "        # Move data to device\n",
    "        atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx = input_data\n",
    "        atom_fea = atom_fea.to(device)\n",
    "        nbr_fea = nbr_fea.to(device)\n",
    "        nbr_fea_idx = nbr_fea_idx.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item() * target.size(0)\n",
    "        predictions.extend(output.detach().cpu().numpy().flatten())\n",
    "        targets.extend(target.detach().cpu().numpy().flatten())\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(targets, predictions))\n",
    "    \n",
    "    return avg_loss, mae, rmse, predictions, targets\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    cif_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation', leave=False)\n",
    "        for batch_idx, (input_data, target, batch_cif_ids) in enumerate(pbar):\n",
    "            # Move data to device\n",
    "            atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx = input_data\n",
    "            atom_fea = atom_fea.to(device)\n",
    "            nbr_fea = nbr_fea.to(device)\n",
    "            nbr_fea_idx = nbr_fea_idx.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item() * target.size(0)\n",
    "            predictions.extend(output.detach().cpu().numpy().flatten())\n",
    "            targets.extend(target.detach().cpu().numpy().flatten())\n",
    "            cif_ids.extend(batch_cif_ids)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(targets, predictions))\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    \n",
    "    return avg_loss, mae, rmse, r2, predictions, targets, cif_ids\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training and validation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad814cea",
   "metadata": {},
   "source": [
    "### Step 8: Training Loop with Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9be4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracking variables\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_mae': [],\n",
    "    'train_rmse': [],\n",
    "    'val_loss': [],\n",
    "    'val_mae': [],\n",
    "    'val_rmse': [],\n",
    "    'val_r2': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "early_stopping_patience = 30\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_mae, train_rmse, _, _ = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_mae, val_rmse, val_r2, val_preds, val_targets, _ = validate_epoch(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_mae'].append(train_mae)\n",
    "    history['train_rmse'].append(train_rmse)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_mae'].append(val_mae)\n",
    "    history['val_rmse'].append(val_rmse)\n",
    "    history['val_r2'].append(val_r2)\n",
    "    history['learning_rate'].append(current_lr)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.6f} | Train MAE: {train_mae:.6f} | Train RMSE: {train_rmse:.6f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.6f} | Val MAE:   {val_mae:.6f} | Val RMSE:   {val_rmse:.6f} | Val R¬≤: {val_r2:.4f}\")\n",
    "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "        print(f\"‚úÖ New best model! Val Loss: {best_val_loss:.6f}\")\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_mae': val_mae,\n",
    "        }, 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(f\"\\n‚ö†Ô∏è Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "    \n",
    "    # Save training progress periodically\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        pd.DataFrame(history).to_csv('training_history.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"‚úÖ Best model loaded\")\n",
    "\n",
    "# Save final history\n",
    "pd.DataFrame(history).to_csv('training_history.csv', index=False)\n",
    "print(\"‚úÖ Training history saved to 'training_history.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797e47f6",
   "metadata": {},
   "source": [
    "### Step 9: Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0554ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive training visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "axes[0, 0].plot(epochs_range, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(epochs_range, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('MSE Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: MAE curves\n",
    "axes[0, 1].plot(epochs_range, history['train_mae'], 'b-', label='Train MAE', linewidth=2)\n",
    "axes[0, 1].plot(epochs_range, history['val_mae'], 'r-', label='Validation MAE', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Mean Absolute Error', fontsize=12)\n",
    "axes[0, 1].set_title('Mean Absolute Error', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: RMSE curves\n",
    "axes[1, 0].plot(epochs_range, history['train_rmse'], 'b-', label='Train RMSE', linewidth=2)\n",
    "axes[1, 0].plot(epochs_range, history['val_rmse'], 'r-', label='Validation RMSE', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Root Mean Squared Error', fontsize=12)\n",
    "axes[1, 0].set_title('Root Mean Squared Error', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: R¬≤ Score and Learning Rate\n",
    "ax1 = axes[1, 1]\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "line1 = ax1.plot(epochs_range, history['val_r2'], 'g-', label='Validation R¬≤', linewidth=2)\n",
    "line2 = ax2.plot(epochs_range, history['learning_rate'], 'orange', label='Learning Rate', \n",
    "                 linewidth=2, linestyle='--')\n",
    "\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('R¬≤ Score', fontsize=12, color='g')\n",
    "ax2.set_ylabel('Learning Rate', fontsize=12, color='orange')\n",
    "ax1.set_title('R¬≤ Score and Learning Rate', fontsize=14, fontweight='bold')\n",
    "ax1.tick_params(axis='y', labelcolor='g')\n",
    "ax2.tick_params(axis='y', labelcolor='orange')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Combine legends\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_progress.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Training progress visualizations saved to 'training_progress.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb7317f",
   "metadata": {},
   "source": [
    "### Step 10: Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35984785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"=\" * 80)\n",
    "print(\"Evaluating on Test Set\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_loss, test_mae, test_rmse, test_r2, test_preds, test_targets, test_cif_ids = validate_epoch(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Test Set Performance:\")\n",
    "print(f\"   Loss (MSE): {test_loss:.6f}\")\n",
    "print(f\"   MAE:        {test_mae:.6f}\")\n",
    "print(f\"   RMSE:       {test_rmse:.6f}\")\n",
    "print(f\"   R¬≤ Score:   {test_r2:.4f}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "test_results = pd.DataFrame({\n",
    "    'material_id': test_cif_ids,\n",
    "    'true_value': test_targets,\n",
    "    'predicted_value': test_preds,\n",
    "    'error': np.array(test_targets) - np.array(test_preds),\n",
    "    'abs_error': np.abs(np.array(test_targets) - np.array(test_preds))\n",
    "})\n",
    "\n",
    "# Sort by absolute error\n",
    "test_results_sorted = test_results.sort_values('abs_error', ascending=False)\n",
    "\n",
    "print(f\"\\nüìã Top 5 Best Predictions:\")\n",
    "print(test_results_sorted.tail(5)[['material_id', 'true_value', 'predicted_value', 'abs_error']])\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Top 5 Worst Predictions:\")\n",
    "print(test_results_sorted.head(5)[['material_id', 'true_value', 'predicted_value', 'abs_error']])\n",
    "\n",
    "# Save test results\n",
    "test_results.to_csv('test_results.csv', index=False)\n",
    "print(f\"\\n‚úÖ Test results saved to 'test_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e677fff5",
   "metadata": {},
   "source": [
    "### Step 11: Prediction Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4158f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive prediction visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Predicted vs True Values (Test Set)\n",
    "axes[0, 0].scatter(test_targets, test_preds, alpha=0.6, s=100, edgecolors='black', linewidth=0.5)\n",
    "axes[0, 0].plot([min(test_targets), max(test_targets)], \n",
    "                [min(test_targets), max(test_targets)], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_xlabel('True Values', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Predicted Values', fontsize=12)\n",
    "axes[0, 0].set_title(f'Test Set: Predicted vs True\\nR¬≤ = {test_r2:.4f}, MAE = {test_mae:.4f}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residual Plot\n",
    "residuals = np.array(test_targets) - np.array(test_preds)\n",
    "axes[0, 1].scatter(test_preds, residuals, alpha=0.6, s=100, edgecolors='black', linewidth=0.5)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Predicted Values', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Residuals (True - Predicted)', fontsize=12)\n",
    "axes[0, 1].set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Error Distribution\n",
    "axes[1, 0].hist(residuals, bins=20, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[1, 0].axvline(x=0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
    "axes[1, 0].set_xlabel('Residuals (True - Predicted)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 0].set_title(f'Error Distribution\\nMean = {np.mean(residuals):.4f}, Std = {np.std(residuals):.4f}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Absolute Error Distribution\n",
    "abs_errors = np.abs(residuals)\n",
    "axes[1, 1].hist(abs_errors, bins=20, edgecolor='black', alpha=0.7, color='lightcoral')\n",
    "axes[1, 1].axvline(x=test_mae, color='r', linestyle='--', linewidth=2, \n",
    "                   label=f'MAE = {test_mae:.4f}')\n",
    "axes[1, 1].set_xlabel('Absolute Error', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].set_title('Absolute Error Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Prediction analysis visualizations saved to 'prediction_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0120f5de",
   "metadata": {},
   "source": [
    "### Step 12: Compare Performance Across All Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb1d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on all splits for comprehensive comparison\n",
    "print(\"Evaluating on all data splits...\")\n",
    "\n",
    "# Get final training set predictions\n",
    "_, train_mae_final, train_rmse_final, _, train_preds_final, train_targets_final = train_epoch(\n",
    "    model, train_loader, criterion, optimizer, device\n",
    ")\n",
    "\n",
    "# Get final validation set predictions\n",
    "_, val_mae_final, val_rmse_final, val_r2_final, val_preds_final, val_targets_final, _ = validate_epoch(\n",
    "    model, val_loader, criterion, device\n",
    ")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "performance_summary = pd.DataFrame({\n",
    "    'Split': ['Training', 'Validation', 'Test'],\n",
    "    'MAE': [train_mae_final, val_mae_final, test_mae],\n",
    "    'RMSE': [train_rmse_final, val_rmse_final, test_rmse],\n",
    "    'R¬≤': [r2_score(train_targets_final, train_preds_final), val_r2_final, test_r2]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(performance_summary.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Visualize performance comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = ['MAE', 'RMSE', 'R¬≤']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    axes[idx].bar(performance_summary['Split'], performance_summary[metric], \n",
    "                  color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    axes[idx].set_ylabel(metric, fontsize=12)\n",
    "    axes[idx].set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(performance_summary[metric]):\n",
    "        axes[idx].text(i, v, f'{v:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Performance comparison saved to 'performance_comparison.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
